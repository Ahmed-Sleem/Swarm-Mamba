{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1379236,"sourceType":"datasetVersion","datasetId":804556},{"sourceId":1873742,"sourceType":"datasetVersion","datasetId":1115384},{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633},{"sourceId":4619805,"sourceType":"datasetVersion","datasetId":2688675}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# POC","metadata":{}},{"cell_type":"code","source":"\"\"\"\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n‚ïë  SWARM-MAMBA: Asynchronous Parallel State Machines with Emergent Consensus   ‚ïë\n‚ïë  Proof of Concept for Q1 Research Validation                                 ‚ïë\n‚ïë                                                                              ‚ïë\n‚ïë  Decision Thresholds:                                                        ‚ïë\n‚ïë  ‚Ä¢ Accuracy Delta: Swarm within 10% of baseline                              ‚ïë\n‚ïë  ‚Ä¢ Speedup: ‚â•1.0x with k=4 agents                                            ‚ïë\n‚ïë  ‚Ä¢ Consensus Rate: ‚â•60%                                                      ‚ïë\n‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport time\nfrom typing import List, Tuple, Dict, Optional\nfrom dataclasses import dataclass\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# CONFIGURATION & CONSTANTS\n# ============================================================================\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"Central configuration for reproducibility\"\"\"\n    seed: int = 42\n    img_size: int = 64\n    patch_size: int = 8\n    n_classes: int = 2\n    n_train: int = 200\n    n_test: int = 56\n    epochs: int = 12\n    batch_size: int = 32\n    lr: float = 1e-3\n    \n    # Decision thresholds\n    accuracy_threshold: float = 0.65\n    relative_accuracy_threshold: float = 0.90\n    speedup_threshold: float = 1.0\n    consensus_threshold: float = 0.60\n\nCONFIG = ExperimentConfig()\n\n# Set seeds\ntorch.manual_seed(CONFIG.seed)\nnp.random.seed(CONFIG.seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(CONFIG.seed)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ============================================================================\n# SECTION 1: SIMPLIFIED SELECTIVE STATE SPACE MODEL (SSM)\n# ============================================================================\n\nclass SelectiveSSM(nn.Module):\n    \"\"\"\n    Simplified Selective State Space Model capturing Mamba's core innovation:\n    Input-dependent selection mechanism for dynamic context modeling.\n    \n    Key insight: The selection mechanism (Œî, B, C) is computed from input,\n    enabling the model to selectively propagate or forget information.\n    \n    Reference: Gu & Dao (2024), \"Mamba: Linear-Time Sequence Modeling\"\n    \"\"\"\n    def __init__(\n        self, \n        d_model: int, \n        d_state: int = 16, \n        expand_factor: int = 2,\n        dt_rank: str = \"auto\"\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_inner = d_model * expand_factor\n        self.dt_rank = d_state if dt_rank == \"auto\" else dt_rank\n        \n        # Input projection: x ‚Üí (x_branch, gate)\n        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n        \n        # Depthwise conv for local context\n        self.conv1d = nn.Conv1d(\n            self.d_inner, self.d_inner,\n            kernel_size=4, padding=2,\n            groups=self.d_inner\n        )\n        \n        # Selection mechanism projections\n        # Projects to (dt, B, C) - all input-dependent\n        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n        \n        # Initialize dt bias for stability\n        dt_init_std = self.dt_rank ** -0.5\n        nn.init.uniform_(self.dt_proj.bias, -dt_init_std, dt_init_std)\n        \n        # Learnable state matrix A (discretized log-space)\n        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n        self.A_log = nn.Parameter(torch.log(A))\n        \n        # Output normalization and projection\n        self.norm = nn.LayerNorm(self.d_inner)\n        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, seq_len, d_model)\n        Returns:\n            (batch, seq_len, d_model)\n        \"\"\"\n        batch, seq_len, _ = x.shape\n        \n        # Step 1: Input projection and split into branches\n        xz = self.in_proj(x)  # (B, L, 2*d_inner)\n        x_branch, z = xz.chunk(2, dim=-1)  # Each: (B, L, d_inner)\n        \n        # Step 2: Local convolution for short-range dependencies\n        x_branch = x_branch.transpose(1, 2)  # (B, d_inner, L)\n        x_branch = self.conv1d(x_branch)[:, :, :seq_len]  # Truncate padding\n        x_branch = x_branch.transpose(1, 2)  # (B, L, d_inner)\n        x_branch = F.silu(x_branch)\n        \n        # Step 3: Compute input-dependent selection parameters\n        x_dbl = self.x_proj(x_branch)  # (B, L, dt_rank + 2*d_state)\n        dt, B, C = torch.split(\n            x_dbl, \n            [self.dt_rank, self.d_state, self.d_state], \n            dim=-1\n        )\n        \n        # Step 4: Discretize dt (step size)\n        dt = F.softplus(self.dt_proj(dt))  # (B, L, d_inner)\n        \n        # Step 5: Compute discretized A\n        A = -torch.exp(self.A_log)  # (d_state,)\n        \n        # Step 6: Selective scan (simplified parallel approximation for PoC)\n        # Full implementation would use associative scan\n        y = self._selective_scan_simple(x_branch, dt, A, B, C)\n        \n        # Step 7: Output gating and projection\n        y = self.norm(y)\n        y = y * F.silu(z)  # Gating mechanism\n        return self.out_proj(y)\n    \n    def _selective_scan_simple(\n        self, \n        x: torch.Tensor,      # (B, L, d_inner)\n        dt: torch.Tensor,     # (B, L, d_inner)\n        A: torch.Tensor,      # (d_state,)\n        B: torch.Tensor,      # (B, L, d_state)\n        C: torch.Tensor       # (B, L, d_state)\n    ) -> torch.Tensor:\n        \"\"\"\n        Simplified selective scan for PoC.\n        Uses chunk-wise processing for memory efficiency.\n        \"\"\"\n        batch, seq_len, d_inner = x.shape\n        d_state = self.d_state\n        \n        # Process in chunks for efficiency\n        chunk_size = min(32, seq_len)\n        n_chunks = (seq_len + chunk_size - 1) // chunk_size\n        \n        outputs = []\n        h = torch.zeros(batch, d_inner, d_state, device=x.device)\n        \n        for c in range(n_chunks):\n            start = c * chunk_size\n            end = min(start + chunk_size, seq_len)\n            \n            for t in range(start, end):\n                # Discretized state update\n                dt_t = dt[:, t, :].unsqueeze(-1)  # (B, d_inner, 1)\n                dA = torch.exp(A.view(1, 1, -1) * dt_t)  # (B, d_inner, d_state)\n                dB = dt_t * B[:, t, :].unsqueeze(1)  # (B, d_inner, d_state)\n                \n                # State update: h = dA * h + dB * x\n                h = dA * h + dB * x[:, t, :].unsqueeze(-1)\n                \n                # Output: y = C @ h\n                y_t = (h * C[:, t, :].unsqueeze(1)).sum(dim=-1)  # (B, d_inner)\n                outputs.append(y_t)\n        \n        return torch.stack(outputs, dim=1)  # (B, L, d_inner)\n\n\n# ============================================================================\n# SECTION 2: MICRO-MAMBA AGENT\n# ============================================================================\n\nclass MicroMamba(nn.Module):\n    \"\"\"\n    Lightweight Mamba agent for swarm processing.\n    Each agent processes a spatial partition of the image.\n    \n    Design principle: Keep individual agents small (O(n/k) parameters)\n    while maintaining expressive power through communication.\n    \"\"\"\n    def __init__(\n        self, \n        d_model: int, \n        n_layers: int = 2, \n        d_state: int = 16,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.n_layers = n_layers\n        \n        # Stack of SSM blocks with residual connections\n        self.blocks = nn.ModuleList()\n        for _ in range(n_layers):\n            self.blocks.append(nn.ModuleDict({\n                'norm': nn.LayerNorm(d_model),\n                'ssm': SelectiveSSM(d_model, d_state),\n                'dropout': nn.Dropout(dropout)\n            }))\n        \n        # Agent-specific learned embedding\n        self.agent_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Process local partition.\n        \n        Args:\n            x: (batch, seq_len, d_model) - partition tokens\n        Returns:\n            (batch, seq_len, d_model) - processed tokens\n        \"\"\"\n        batch = x.shape[0]\n        \n        # Prepend agent token for identity-aware processing\n        agent_tok = self.agent_token.expand(batch, -1, -1)\n        x = torch.cat([agent_tok, x], dim=1)\n        \n        # Process through SSM blocks\n        for block in self.blocks:\n            residual = x\n            x = block['norm'](x)\n            x = block['ssm'](x)\n            x = block['dropout'](x)\n            x = residual + x\n        \n        # Remove agent token\n        return x[:, 1:, :]\n    \n    def get_summary_state(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Get compressed state for inter-agent communication\"\"\"\n        return x.mean(dim=1)  # (batch, d_model)\n\n\n# ============================================================================\n# SECTION 3: INTER-AGENT COMMUNICATION\n# ============================================================================\n\nclass DifferentiableMessagePassing(nn.Module):\n    \"\"\"\n    Learned message passing between swarm agents.\n    \n    Key innovation: Agents broadcast compressed states and aggregate\n    messages through learned attention, enabling emergent coordination.\n    \n    This mimics cortical column communication in biological vision.\n    \"\"\"\n    def __init__(\n        self, \n        d_model: int, \n        n_agents: int,\n        compression_ratio: int = 2,\n        n_heads: int = 4\n    ):\n        super().__init__()\n        self.n_agents = n_agents\n        self.d_model = d_model\n        self.d_compressed = d_model // compression_ratio\n        \n        # State compression/decompression\n        self.compress = nn.Sequential(\n            nn.Linear(d_model, self.d_compressed),\n            nn.GELU(),\n            nn.LayerNorm(self.d_compressed)\n        )\n        self.decompress = nn.Linear(self.d_compressed, d_model)\n        \n        # Multi-head cross-agent attention\n        self.n_heads = n_heads\n        self.head_dim = d_model // n_heads\n        \n        self.q_proj = nn.Linear(d_model, d_model)\n        self.k_proj = nn.Linear(d_model, d_model)\n        self.v_proj = nn.Linear(d_model, d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n        \n        # Learnable mixing coefficient\n        self.alpha = nn.Parameter(torch.tensor(0.3))\n        \n        # Topology mask (optional: can implement local connectivity)\n        self.register_buffer(\n            'topology_mask',\n            torch.ones(n_agents, n_agents) - torch.eye(n_agents)\n        )\n        \n    def forward(self, agent_states: List[torch.Tensor]) -> List[torch.Tensor]:\n        \"\"\"\n        Exchange messages between agents.\n        \n        Args:\n            agent_states: List of (batch, seq_len, d_model) tensors\n        Returns:\n            Updated agent states after message exchange\n        \"\"\"\n        batch = agent_states[0].shape[0]\n        \n        # Step 1: Compress each agent's state for communication\n        summaries = [self.compress(s.mean(dim=1)) for s in agent_states]\n        stacked_summaries = torch.stack(summaries, dim=1)  # (B, n_agents, d_compressed)\n        \n        # Step 2: Decompress for attention computation\n        decompressed = self.decompress(stacked_summaries)  # (B, n_agents, d_model)\n        \n        # Step 3: Multi-head attention across agents\n        Q = self.q_proj(decompressed)  # (B, n_agents, d_model)\n        K = self.k_proj(decompressed)\n        V = self.v_proj(decompressed)\n        \n        # Reshape for multi-head\n        Q = Q.view(batch, self.n_agents, self.n_heads, self.head_dim).transpose(1, 2)\n        K = K.view(batch, self.n_agents, self.n_heads, self.head_dim).transpose(1, 2)\n        V = V.view(batch, self.n_agents, self.n_heads, self.head_dim).transpose(1, 2)\n        \n        # Scaled dot-product attention\n        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.head_dim)\n        \n        # Apply topology mask (agents don't attend to themselves)\n        mask = self.topology_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, n_agents, n_agents)\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V)  # (B, n_heads, n_agents, head_dim)\n        \n        # Reshape back\n        attn_output = attn_output.transpose(1, 2).reshape(batch, self.n_agents, self.d_model)\n        messages = self.out_proj(attn_output)  # (B, n_agents, d_model)\n        \n        # Step 4: Update agent states with received messages\n        alpha = torch.sigmoid(self.alpha)\n        updated_states = []\n        \n        for i, state in enumerate(agent_states):\n            # Broadcast message to all tokens in the partition\n            msg = messages[:, i:i+1, :].expand(-1, state.shape[1], -1)\n            updated = state + alpha * msg\n            updated_states.append(updated)\n        \n        return updated_states\n\n\n# ============================================================================\n# SECTION 4: CONSENSUS MODULE\n# ============================================================================\n\nclass EmergentConsensus(nn.Module):\n    \"\"\"\n    Aggregates agent predictions into unified output.\n    \n    Two consensus mechanisms:\n    1. Learned weighted voting (soft attention over agents)\n    2. Contrastive alignment (agents trained to agree)\n    \n    Also tracks individual agent predictions for interpretability.\n    \"\"\"\n    def __init__(self, d_model: int, n_agents: int, n_classes: int):\n        super().__init__()\n        self.n_agents = n_agents\n        self.d_model = d_model\n        self.n_classes = n_classes\n        \n        # Learnable agent importance weights\n        self.agent_importance = nn.Parameter(torch.ones(n_agents) / n_agents)\n        \n        # Per-agent classification heads (for diversity and interpretability)\n        self.agent_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Linear(d_model, d_model // 2),\n                nn.GELU(),\n                nn.Dropout(0.1),\n                nn.Linear(d_model // 2, n_classes)\n            )\n            for _ in range(n_agents)\n        ])\n        \n        # Unified consensus head (processes concatenated representations)\n        self.consensus_head = nn.Sequential(\n            nn.LayerNorm(d_model * n_agents),\n            nn.Linear(d_model * n_agents, d_model),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model, n_classes)\n        )\n        \n    def forward(\n        self, \n        agent_states: List[torch.Tensor]\n    ) -> Tuple[torch.Tensor, List[torch.Tensor], float]:\n        \"\"\"\n        Compute consensus prediction.\n        \n        Returns:\n            consensus_logits: (batch, n_classes)\n            individual_logits: List of (batch, n_classes)\n            consensus_rate: Scalar measuring agent agreement\n        \"\"\"\n        # Pool each agent's state to single vector\n        pooled_states = [s.mean(dim=1) for s in agent_states]  # List of (B, d_model)\n        \n        # Individual agent predictions\n        individual_logits = [\n            head(state) for head, state in zip(self.agent_heads, pooled_states)\n        ]\n        \n        # Method 1: Weighted average of agent states\n        weights = F.softmax(self.agent_importance, dim=0)\n        weighted_state = sum(w * s for w, s in zip(weights, pooled_states))\n        \n        # Method 2: Concatenated consensus\n        concat_states = torch.cat(pooled_states, dim=-1)  # (B, d_model * n_agents)\n        consensus_logits = self.consensus_head(concat_states)\n        \n        # Compute consensus rate (how much agents agree)\n        with torch.no_grad():\n            individual_preds = torch.stack([\n                torch.argmax(logits, dim=-1) for logits in individual_logits\n            ], dim=1)  # (B, n_agents)\n            \n            # Mode prediction\n            mode_pred = torch.mode(individual_preds, dim=1).values  # (B,)\n            agreement = (individual_preds == mode_pred.unsqueeze(1)).float().mean()\n        \n        return consensus_logits, individual_logits, agreement.item()\n\n\n# ============================================================================\n# SECTION 5: SWARM-MAMBA MAIN ARCHITECTURE\n# ============================================================================\n\nclass SwarmMamba(nn.Module):\n    \"\"\"\n    Swarm-Mamba: Asynchronous Parallel State Machines with Emergent Consensus\n    \n    Architecture:\n    1. Patch embedding (shared across agents)\n    2. Spatial partitioning among k agents\n    3. Parallel processing by micro-Mambas\n    4. Iterative message passing rounds\n    5. Consensus aggregation\n    \n    Key benefits:\n    - True O(n/k) parallel time complexity per agent\n    - Graceful degradation (built-in redundancy)\n    - Interpretable agent specialization\n    \"\"\"\n    def __init__(\n        self,\n        img_size: int = 64,\n        patch_size: int = 8,\n        in_channels: int = 3,\n        n_agents: int = 4,\n        d_model: int = 64,\n        n_layers: int = 2,\n        n_classes: int = 2,\n        n_comm_rounds: int = 2,\n        d_state: int = 16\n    ):\n        super().__init__()\n        self.n_agents = n_agents\n        self.n_comm_rounds = n_comm_rounds\n        self.d_model = d_model\n        self.img_size = img_size\n        self.patch_size = patch_size\n        \n        # Calculate patch grid\n        self.n_patches_side = img_size // patch_size\n        self.n_patches = self.n_patches_side ** 2\n        \n        # Patch embedding (Conv2D based)\n        self.patch_embed = nn.Conv2d(\n            in_channels, d_model,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n        \n        # Learnable position embeddings\n        self.pos_embed = nn.Parameter(\n            torch.randn(1, self.n_patches, d_model) * 0.02\n        )\n        \n        # Swarm of micro-Mamba agents\n        self.agents = nn.ModuleList([\n            MicroMamba(d_model, n_layers, d_state)\n            for _ in range(n_agents)\n        ])\n        \n        # Inter-agent communication\n        self.message_passing = DifferentiableMessagePassing(d_model, n_agents)\n        \n        # Consensus module\n        self.consensus = EmergentConsensus(d_model, n_agents, n_classes)\n        \n    def partition_spatially(self, x: torch.Tensor) -> List[torch.Tensor]:\n        \"\"\"\n        Partition patch sequence among agents using spatial regions.\n        \n        For 2D grid partitioning (more biologically plausible than random):\n        - Divide image into k spatial quadrants/regions\n        - Each agent processes patches from its region\n        \"\"\"\n        batch, n_patches, d = x.shape\n        \n        # Calculate patches per agent\n        patches_per_agent = n_patches // self.n_agents\n        \n        partitions = []\n        for i in range(self.n_agents):\n            start_idx = i * patches_per_agent\n            end_idx = start_idx + patches_per_agent if i < self.n_agents - 1 else n_patches\n            partitions.append(x[:, start_idx:end_idx, :].clone())\n        \n        return partitions\n    \n    def forward(\n        self, \n        x: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[torch.Tensor], float]:\n        \"\"\"\n        Forward pass with parallel agent processing.\n        \n        Args:\n            x: (batch, channels, height, width)\n            \n        Returns:\n            logits: (batch, n_classes) - consensus prediction\n            individual_logits: List of agent predictions\n            consensus_rate: Agreement metric\n        \"\"\"\n        batch = x.shape[0]\n        \n        # Step 1: Patch embedding\n        x = self.patch_embed(x)  # (B, d_model, H', W')\n        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, d_model)\n        x = x + self.pos_embed\n        \n        # Step 2: Partition among agents\n        partitions = self.partition_spatially(x)\n        \n        # Step 3: Initial parallel processing\n        # Note: In production, this would use torch.multiprocessing or GPU parallelism\n        agent_states = []\n        for agent, partition in zip(self.agents, partitions):\n            state = agent(partition)\n            agent_states.append(state)\n        \n        # Step 4: Communication rounds\n        for round_idx in range(self.n_comm_rounds):\n            # Message exchange\n            agent_states = self.message_passing(agent_states)\n            \n            # Additional local processing after receiving messages\n            agent_states = [\n                agent(state) \n                for agent, state in zip(self.agents, agent_states)\n            ]\n        \n        # Step 5: Consensus\n        consensus_logits, individual_logits, consensus_rate = self.consensus(agent_states)\n        \n        return consensus_logits, individual_logits, consensus_rate\n\n\n# ============================================================================\n# SECTION 6: BASELINE SINGLE MAMBA (FOR COMPARISON)\n# ============================================================================\n\nclass SingleMamba(nn.Module):\n    \"\"\"\n    Baseline: Single monolithic Mamba model.\n    Parameters scaled to match total Swarm-Mamba params for fair comparison.\n    \"\"\"\n    def __init__(\n        self,\n        img_size: int = 64,\n        patch_size: int = 8,\n        in_channels: int = 3,\n        d_model: int = 128,\n        n_layers: int = 4,\n        n_classes: int = 2,\n        d_state: int = 16\n    ):\n        super().__init__()\n        \n        self.n_patches = (img_size // patch_size) ** 2\n        \n        # Patch embedding\n        self.patch_embed = nn.Conv2d(\n            in_channels, d_model,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n        \n        # Position embedding\n        self.pos_embed = nn.Parameter(\n            torch.randn(1, self.n_patches, d_model) * 0.02\n        )\n        \n        # SSM blocks\n        self.blocks = nn.ModuleList()\n        for _ in range(n_layers):\n            self.blocks.append(nn.ModuleDict({\n                'norm': nn.LayerNorm(d_model),\n                'ssm': SelectiveSSM(d_model, d_state),\n            }))\n        \n        # Classification head\n        self.head = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, n_classes)\n        )\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Patch embedding\n        x = self.patch_embed(x)\n        x = x.flatten(2).transpose(1, 2)\n        x = x + self.pos_embed\n        \n        # Process through SSM blocks\n        for block in self.blocks:\n            residual = x\n            x = block['norm'](x)\n            x = block['ssm'](x)\n            x = residual + x\n        \n        # Global average pooling + classification\n        x = x.mean(dim=1)\n        return self.head(x)\n\n\n# ============================================================================\n# SECTION 7: SYNTHETIC DATA GENERATION\n# ============================================================================\n\ndef generate_medical_imaging_dataset(\n    n_samples: int,\n    img_size: int = 64,\n    n_classes: int = 2\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Generate synthetic dataset mimicking histopathology patterns.\n    \n    Class 0 (Benign): Uniform cellular texture\n    Class 1 (Malignant): Irregular patterns with focal lesions\n    \n    This is for PoC only - real experiments use BreaKHis dataset.\n    \"\"\"\n    images = []\n    labels = []\n    \n    for i in range(n_samples):\n        label = i % n_classes\n        \n        # Base tissue texture\n        img = torch.randn(3, img_size, img_size) * 0.15\n        \n        # Add class-specific patterns\n        if label == 0:  # Benign\n            # Uniform coloring with slight variations\n            base_color = torch.randn(3, 1, 1) * 0.3 + 0.5\n            img = img + base_color\n            \n            # Regular cellular pattern\n            for cx in range(8, img_size - 8, 12):\n                for cy in range(8, img_size - 8, 12):\n                    y, x = torch.meshgrid(\n                        torch.arange(img_size), \n                        torch.arange(img_size),\n                        indexing='ij'\n                    )\n                    dist = ((x - cx)**2 + (y - cy)**2).float()\n                    mask = dist < 16\n                    img[0, mask] += 0.1\n                    \n        else:  # Malignant\n            # Irregular base coloring\n            img = img + 0.4\n            \n            # Add irregular lesion spots (key diagnostic feature)\n            n_lesions = np.random.randint(3, 8)\n            for _ in range(n_lesions):\n                cx = np.random.randint(10, img_size - 10)\n                cy = np.random.randint(10, img_size - 10)\n                \n                # Irregular shape (ellipse + noise)\n                rx = np.random.randint(4, 12)\n                ry = np.random.randint(4, 12)\n                \n                y, x = torch.meshgrid(\n                    torch.arange(img_size),\n                    torch.arange(img_size),\n                    indexing='ij'\n                )\n                dist = ((x - cx)**2 / rx**2 + (y - cy)**2 / ry**2).float()\n                mask = dist < 1\n                \n                # Darker lesion center\n                intensity = torch.randn(1) * 0.2 - 0.3\n                for c in range(3):\n                    img[c, mask] += intensity + torch.randn(1) * 0.1\n        \n        # Normalize to [0, 1] range\n        img = torch.clamp(img, 0, 1)\n        \n        images.append(img)\n        labels.append(label)\n    \n    return torch.stack(images), torch.tensor(labels, dtype=torch.long)\n\n\n# ============================================================================\n# SECTION 8: TRAINING & EVALUATION\n# ============================================================================\n\ndef train_model(\n    model: nn.Module,\n    train_data: Tuple[torch.Tensor, torch.Tensor],\n    epochs: int,\n    is_swarm: bool = False,\n    verbose: bool = True\n) -> nn.Module:\n    \"\"\"Train model with standard CE loss + optional consistency regularization.\"\"\"\n    \n    X_train, y_train = train_data\n    X_train, y_train = X_train.to(DEVICE), y_train.to(DEVICE)\n    model = model.to(DEVICE)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    criterion = nn.CrossEntropyLoss()\n    \n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        n_batches = 0\n        \n        # Shuffle data\n        perm = torch.randperm(len(X_train))\n        X_train, y_train = X_train[perm], y_train[perm]\n        \n        for i in range(0, len(X_train), CONFIG.batch_size):\n            batch_x = X_train[i:i + CONFIG.batch_size]\n            batch_y = y_train[i:i + CONFIG.batch_size]\n            \n            optimizer.zero_grad()\n            \n            if is_swarm:\n                logits, individual_logits, _ = model(batch_x)\n                \n                # Primary loss\n                loss = criterion(logits, batch_y)\n                \n                # Consistency loss: all agents should predict same class\n                for ind_logits in individual_logits:\n                    loss += 0.1 * criterion(ind_logits, batch_y)\n                    \n                # Diversity regularization: prevent agent collapse\n                # (agents should use different features)\n                agent_features = [model.agents[j].agent_token for j in range(len(model.agents))]\n                if len(agent_features) > 1:\n                    for j in range(len(agent_features) - 1):\n                        sim = F.cosine_similarity(\n                            agent_features[j].flatten(),\n                            agent_features[j+1].flatten(),\n                            dim=0\n                        )\n                        loss += 0.01 * sim  # Minimize similarity\n            else:\n                logits = model(batch_x)\n                loss = criterion(logits, batch_y)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            total_loss += loss.item()\n            n_batches += 1\n        \n        scheduler.step()\n        \n        if verbose and (epoch + 1) % 4 == 0:\n            avg_loss = total_loss / n_batches\n            print(f\"    Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n    \n    return model\n\n\ndef evaluate_model(\n    model: nn.Module,\n    test_data: Tuple[torch.Tensor, torch.Tensor],\n    is_swarm: bool = False\n) -> Dict[str, float]:\n    \"\"\"Evaluate model on test set.\"\"\"\n    \n    X_test, y_test = test_data\n    X_test, y_test = X_test.to(DEVICE), y_test.to(DEVICE)\n    model = model.to(DEVICE)\n    \n    model.eval()\n    correct = 0\n    total_time = 0\n    consensus_rates = []\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for i in range(len(X_test)):\n            x = X_test[i:i+1]\n            \n            # Time the inference\n            if DEVICE.type == 'cuda':\n                torch.cuda.synchronize()\n            start = time.perf_counter()\n            \n            if is_swarm:\n                logits, _, cons_rate = model(x)\n                consensus_rates.append(cons_rate)\n            else:\n                logits = model(x)\n            \n            if DEVICE.type == 'cuda':\n                torch.cuda.synchronize()\n            end = time.perf_counter()\n            \n            total_time += (end - start) * 1000  # Convert to ms\n            \n            pred = torch.argmax(logits, dim=-1)\n            correct += (pred == y_test[i]).sum().item()\n            \n            all_preds.append(pred.cpu().item())\n            all_labels.append(y_test[i].cpu().item())\n    \n    accuracy = correct / len(X_test)\n    avg_time_ms = total_time / len(X_test)\n    avg_consensus = np.mean(consensus_rates) if consensus_rates else 1.0\n    \n    # Per-class accuracy\n    class_acc = {}\n    for c in range(CONFIG.n_classes):\n        mask = [l == c for l in all_labels]\n        if sum(mask) > 0:\n            class_correct = sum(p == l for p, l, m in zip(all_preds, all_labels, mask) if m)\n            class_acc[f'class_{c}_acc'] = class_correct / sum(mask)\n    \n    return {\n        'accuracy': accuracy,\n        'inference_time_ms': avg_time_ms,\n        'consensus_rate': avg_consensus,\n        **class_acc\n    }\n\n\n# ============================================================================\n# SECTION 9: TEST SUITE\n# ============================================================================\n\ndef run_test_suite() -> bool:\n    \"\"\"Comprehensive test suite for all components.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\" RUNNING TEST SUITE\")\n    print(\"=\" * 70)\n    \n    all_passed = True\n    \n    # Test 1: SelectiveSSM\n    print(\"\\n[TEST 1] SelectiveSSM forward pass...\")\n    try:\n        ssm = SelectiveSSM(d_model=32, d_state=8).to(DEVICE)\n        x = torch.randn(2, 16, 32).to(DEVICE)\n        out = ssm(x)\n        assert out.shape == x.shape, f\"Shape mismatch: {out.shape} vs {x.shape}\"\n        assert not torch.isnan(out).any(), \"NaN in output\"\n        print(\"  ‚úì Output shape and values correct\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    # Test 2: MicroMamba\n    print(\"\\n[TEST 2] MicroMamba agent processing...\")\n    try:\n        agent = MicroMamba(d_model=32, n_layers=2).to(DEVICE)\n        x = torch.randn(2, 8, 32).to(DEVICE)\n        out = agent(x)\n        assert out.shape == x.shape, f\"Shape mismatch\"\n        print(\"  ‚úì Agent processes partition correctly\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    # Test 3: Message Passing\n    print(\"\\n[TEST 3] Inter-agent message passing...\")\n    try:\n        mp = DifferentiableMessagePassing(d_model=32, n_agents=4).to(DEVICE)\n        states = [torch.randn(2, 8, 32).to(DEVICE) for _ in range(4)]\n        updated = mp(states)\n        assert len(updated) == 4, \"Wrong number of states returned\"\n        assert all(u.shape == s.shape for u, s in zip(updated, states))\n        # Check that states actually changed (communication happened)\n        changed = any(not torch.allclose(u, s) for u, s in zip(updated, states))\n        assert changed, \"States unchanged after message passing\"\n        print(\"  ‚úì Message passing modifies states correctly\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    # Test 4: Consensus Module\n    print(\"\\n[TEST 4] Emergent consensus...\")\n    try:\n        cons = EmergentConsensus(d_model=32, n_agents=4, n_classes=2).to(DEVICE)\n        states = [torch.randn(2, 8, 32).to(DEVICE) for _ in range(4)]\n        logits, ind_logits, rate = cons(states)\n        assert logits.shape == (2, 2), f\"Wrong consensus shape: {logits.shape}\"\n        assert len(ind_logits) == 4, \"Wrong number of individual predictions\"\n        assert 0 <= rate <= 1, f\"Invalid consensus rate: {rate}\"\n        print(f\"  ‚úì Consensus produces valid output (rate: {rate:.2f})\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    # Test 5: Full SwarmMamba\n    print(\"\\n[TEST 5] SwarmMamba end-to-end...\")\n    try:\n        swarm = SwarmMamba(\n            n_agents=4, d_model=32, n_layers=2, n_comm_rounds=2\n        ).to(DEVICE)\n        img = torch.randn(2, 3, 64, 64).to(DEVICE)\n        logits, ind_logits, cons = swarm(img)\n        assert logits.shape == (2, 2)\n        assert len(ind_logits) == 4\n        print(f\"  ‚úì SwarmMamba produces valid predictions\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    # Test 6: Parameter count sanity\n    print(\"\\n[TEST 6] Parameter efficiency check...\")\n    try:\n        swarm = SwarmMamba(n_agents=4, d_model=64, n_layers=2)\n        single = SingleMamba(d_model=128, n_layers=4)\n        \n        swarm_params = sum(p.numel() for p in swarm.parameters())\n        single_params = sum(p.numel() for p in single.parameters())\n        \n        ratio = swarm_params / single_params\n        print(f\"  Swarm params: {swarm_params:,}\")\n        print(f\"  Single params: {single_params:,}\")\n        print(f\"  Ratio: {ratio:.2f}x\")\n        \n        # Swarm should be comparable (within 3x)\n        assert ratio < 3.0, f\"Swarm too large: {ratio:.2f}x baseline\"\n        print(\"  ‚úì Parameter counts reasonable\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    # Test 7: Gradient flow\n    print(\"\\n[TEST 7] Gradient flow through architecture...\")\n    try:\n        swarm = SwarmMamba(n_agents=4, d_model=32).to(DEVICE)\n        img = torch.randn(1, 3, 64, 64, requires_grad=True).to(DEVICE)\n        logits, _, _ = swarm(img)\n        loss = logits.sum()\n        loss.backward()\n        \n        # Check gradients exist\n        has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 \n                      for p in swarm.parameters())\n        assert has_grad, \"No gradients flowing\"\n        print(\"  ‚úì Gradients flow correctly\")\n    except Exception as e:\n        print(f\"  ‚úó FAILED: {e}\")\n        all_passed = False\n    \n    print(\"\\n\" + \"=\" * 70)\n    if all_passed:\n        print(\" ALL TESTS PASSED ‚úì\")\n    else:\n        print(\" SOME TESTS FAILED ‚úó\")\n    print(\"=\" * 70)\n    \n    return all_passed\n\n\n# ============================================================================\n# SECTION 10: MAIN EXPERIMENT\n# ============================================================================\n\ndef main():\n    \"\"\"Main experimental pipeline for PoC validation.\"\"\"\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\" SWARM-MAMBA: PROOF OF CONCEPT VALIDATION\")\n    print(\" Asynchronous Parallel State Machines with Emergent Consensus\")\n    print(\"=\" * 70)\n    print(f\"\\n Device: {DEVICE}\")\n    print(f\" Config: {CONFIG}\")\n    \n    # Run test suite\n    tests_passed = run_test_suite()\n    if not tests_passed:\n        print(\"\\n‚ùå CRITICAL: Tests failed. Aborting experiment.\")\n        return \"NO-GO\", {}\n    \n    # Generate data\n    print(\"\\n\" + \"-\" * 50)\n    print(\" DATA GENERATION\")\n    print(\"-\" * 50)\n    print(f\" Generating synthetic medical imaging dataset...\")\n    \n    X_train, y_train = generate_medical_imaging_dataset(CONFIG.n_train, CONFIG.img_size)\n    X_test, y_test = generate_medical_imaging_dataset(CONFIG.n_test, CONFIG.img_size)\n    \n    print(f\"  Training samples: {len(X_train)}\")\n    print(f\"  Test samples: {len(X_test)}\")\n    print(f\"  Image size: {CONFIG.img_size}x{CONFIG.img_size}x3\")\n    print(f\"  Classes: {CONFIG.n_classes} (benign/malignant)\")\n    print(f\"  Class balance: {(y_train == 0).sum().item()}/{(y_train == 1).sum().item()}\")\n    \n    results = {}\n    \n    # ========================================\n    # Experiment 1: Single Mamba Baseline\n    # ========================================\n    print(\"\\n\" + \"-\" * 50)\n    print(\" EXPERIMENT 1: Single Mamba (Baseline)\")\n    print(\"-\" * 50)\n    \n    single_model = SingleMamba(d_model=128, n_layers=4)\n    single_params = sum(p.numel() for p in single_model.parameters())\n    print(f\"  Parameters: {single_params:,}\")\n    \n    print(\"  Training...\")\n    single_model = train_model(\n        single_model, (X_train, y_train), CONFIG.epochs, is_swarm=False\n    )\n    \n    print(\"  Evaluating...\")\n    single_results = evaluate_model(single_model, (X_test, y_test), is_swarm=False)\n    results['single'] = single_results\n    \n    print(f\"\\n  Results:\")\n    print(f\"    Accuracy: {single_results['accuracy']:.1%}\")\n    print(f\"    Inference time: {single_results['inference_time_ms']:.2f} ms/sample\")\n    \n    # ========================================\n    # Experiment 2: Swarm-Mamba (k=4)\n    # ========================================\n    print(\"\\n\" + \"-\" * 50)\n    print(\" EXPERIMENT 2: Swarm-Mamba (k=4 agents)\")\n    print(\"-\" * 50)\n    \n    swarm_model = SwarmMamba(\n        n_agents=4, d_model=64, n_layers=2, n_comm_rounds=2\n    )\n    swarm_params = sum(p.numel() for p in swarm_model.parameters())\n    print(f\"  Parameters: {swarm_params:,}\")\n    print(f\"  Parameter ratio: {swarm_params/single_params:.2f}x baseline\")\n    \n    print(\"  Training...\")\n    swarm_model = train_model(\n        swarm_model, (X_train, y_train), CONFIG.epochs, is_swarm=True\n    )\n    \n    print(\"  Evaluating...\")\n    swarm_results = evaluate_model(swarm_model, (X_test, y_test), is_swarm=True)\n    results['swarm_4'] = swarm_results\n    \n    print(f\"\\n  Results:\")\n    print(f\"    Accuracy: {swarm_results['accuracy']:.1%}\")\n    print(f\"    Inference time: {swarm_results['inference_time_ms']:.2f} ms/sample\")\n    print(f\"    Consensus rate: {swarm_results['consensus_rate']:.1%}\")\n    \n    # ========================================\n    # Experiment 3: Agent Scaling\n    # ========================================\n    print(\"\\n\" + \"-\" * 50)\n    print(\" EXPERIMENT 3: Agent Scaling Analysis\")\n    print(\"-\" * 50)\n    \n    for k in [2, 8]:\n        print(f\"\\n  Testing k={k} agents...\")\n        model = SwarmMamba(\n            n_agents=k, d_model=64, n_layers=2, n_comm_rounds=2\n        )\n        model = train_model(\n            model, (X_train, y_train), CONFIG.epochs // 2, \n            is_swarm=True, verbose=False\n        )\n        res = evaluate_model(model, (X_test, y_test), is_swarm=True)\n        results[f'swarm_{k}'] = res\n        print(f\"    k={k}: Acc={res['accuracy']:.1%}, \"\n              f\"Time={res['inference_time_ms']:.2f}ms, \"\n              f\"Consensus={res['consensus_rate']:.1%}\")\n    \n    # ========================================\n    # Decision Analysis\n    # ========================================\n    print(\"\\n\" + \"=\" * 70)\n    print(\" EVALUATION RESULTS & DECISION\")\n    print(\"=\" * 70)\n    \n    # Calculate key metrics\n    swarm_acc = results['swarm_4']['accuracy']\n    single_acc = results['single']['accuracy']\n    relative_accuracy = swarm_acc / single_acc if single_acc > 0 else 0\n    \n    swarm_time = results['swarm_4']['inference_time_ms']\n    single_time = results['single']['inference_time_ms']\n    speedup = single_time / swarm_time if swarm_time > 0 else 0\n    \n    consensus = results['swarm_4']['consensus_rate']\n    \n    # Decision criteria\n    criteria = [\n        (\n            \"Absolute Accuracy\",\n            swarm_acc >= CONFIG.accuracy_threshold,\n            f\"{swarm_acc:.1%}\",\n            f\"‚â•{CONFIG.accuracy_threshold:.0%}\"\n        ),\n        (\n            \"Relative Accuracy\",\n            relative_accuracy >= CONFIG.relative_accuracy_threshold,\n            f\"{relative_accuracy:.1%}\",\n            f\"‚â•{CONFIG.relative_accuracy_threshold:.0%}\"\n        ),\n        (\n            \"Inference Speed\",\n            speedup >= CONFIG.speedup_threshold,\n            f\"{speedup:.2f}x\",\n            f\"‚â•{CONFIG.speedup_threshold:.1f}x\"\n        ),\n        (\n            \"Agent Consensus\",\n            consensus >= CONFIG.consensus_threshold,\n            f\"{consensus:.1%}\",\n            f\"‚â•{CONFIG.consensus_threshold:.0%}\"\n        ),\n    ]\n    \n    # Print decision table\n    print(\"\\n‚îå\" + \"‚îÄ\" * 25 + \"‚î¨\" + \"‚îÄ\" * 12 + \"‚î¨\" + \"‚îÄ\" * 12 + \"‚î¨\" + \"‚îÄ\" * 10 + \"‚îê\")\n    print(f\"‚îÇ{'Criterion':<25}‚îÇ{'Achieved':^12}‚îÇ{'Threshold':^12}‚îÇ{'Status':^10}‚îÇ\")\n    print(\"‚îú\" + \"‚îÄ\" * 25 + \"‚îº\" + \"‚îÄ\" * 12 + \"‚îº\" + \"‚îÄ\" * 12 + \"‚îº\" + \"‚îÄ\" * 10 + \"‚î§\")\n    \n    passes = 0\n    for name, passed, achieved, threshold in criteria:\n        status = \"‚úì PASS\" if passed else \"‚úó FAIL\"\n        passes += int(passed)\n        print(f\"‚îÇ{name:<25}‚îÇ{achieved:^12}‚îÇ{threshold:^12}‚îÇ{status:^10}‚îÇ\")\n    \n    print(\"‚îî\" + \"‚îÄ\" * 25 + \"‚î¥\" + \"‚îÄ\" * 12 + \"‚î¥\" + \"‚îÄ\" * 12 + \"‚î¥\" + \"‚îÄ\" * 10 + \"‚îò\")\n    \n    # Scaling analysis\n    print(\"\\n Agent Scaling Analysis:\")\n    scaling_results = [(k, results[f'swarm_{k}']['accuracy']) \n                       for k in [2, 4, 8] if f'swarm_{k}' in results]\n    for k, acc in scaling_results:\n        print(f\"    k={k}: {acc:.1%}\")\n    \n    # Check monotonicity\n    accs = [r[1] for r in scaling_results]\n    is_monotonic = all(accs[i] <= accs[i+1] for i in range(len(accs)-1))\n    \n    # ========================================\n    # Final Verdict\n    # ========================================\n    print(\"\\n\" + \"=\" * 70)\n    print(\" STRATEGIC VERDICT\")\n    print(\"=\" * 70)\n    \n    if passes >= 4:\n        verdict = \"GO\"\n        recommendation = \"PROCEED TO FULL IMPLEMENTATION ON REAL DATA\"\n        icon = \"üü¢\"\n    elif passes >= 2:\n        verdict = \"IMPROVE\"\n        recommendation = \"REFINE ARCHITECTURE - ADDRESS IDENTIFIED GAPS\"\n        icon = \"üü°\"\n    else:\n        verdict = \"NO-GO\"\n        recommendation = \"FUNDAMENTAL RETHINK REQUIRED\"\n        icon = \"üî¥\"\n    \n    print(f\"\\n  {icon} FINAL VERDICT: {verdict}\")\n    print(f\"  üìä Criteria Passed: {passes}/4\")\n    print(f\"  üìã Recommendation: {recommendation}\")\n    \n    # Key insights\n    print(\"\\n\" + \"-\" * 50)\n    print(\" KEY INSIGHTS\")\n    print(\"-\" * 50)\n    \n    insights = []\n    \n    if relative_accuracy >= 0.95:\n        insights.append(\"‚úì Distributed processing successfully maintains accuracy\")\n    else:\n        insights.append(f\"‚ö† Accuracy gap ({(1-relative_accuracy)*100:.1f}%) - \"\n                       \"consider deeper message passing\")\n    \n    if consensus >= 0.7:\n        insights.append(\"‚úì High agent consensus indicates stable learned representations\")\n    else:\n        insights.append(\"‚ö† Low consensus - add stronger alignment regularization\")\n    \n    if speedup < 1.0:\n        insights.append(\"‚ö† Communication overhead exceeds parallelization gain - \"\n                       \"optimize message compression\")\n    \n    if not is_monotonic:\n        insights.append(\"‚ö† Non-monotonic scaling suggests communication saturation\")\n    \n    for insight in insights:\n        print(f\"  {insight}\")\n    \n    # Technical recommendations\n    print(\"\\n\" + \"-\" * 50)\n    print(\" TECHNICAL RECOMMENDATIONS FOR Q1\")\n    print(\"-\" * 50)\n    \n    if verdict == \"GO\":\n        recs = [\n            \"1. Implement true GPU parallelism via torch.multiprocessing\",\n            \"2. Test on BreaKHis 400X with ResNet-pretrained patch embeddings\",\n            \"3. Add curriculum learning: start with k=2, gradually increase\",\n            \"4. Benchmark against MambaMIL and TransMIL baselines\"\n        ]\n    elif verdict == \"IMPROVE\":\n        recs = [\n            \"1. Reduce message passing overhead via sparse communication\",\n            \"2. Implement asynchronous update (not all agents sync every round)\",\n            \"3. Add contrastive loss between agent representations\",\n            \"4. Experiment with hierarchical agent topology (tree vs. full)\"\n        ]\n    else:\n        recs = [\n            \"1. Revisit core hypothesis - is parallelism the right approach?\",\n            \"2. Consider sequential multi-agent (not parallel)\",\n            \"3. Try simpler consensus: just average logits\"\n        ]\n    \n    for rec in recs:\n        print(f\"  {rec}\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(f\" Q1 RESEARCH DECISION: {verdict}\")\n    print(\"=\" * 70)\n    \n    return verdict, results\n\n\n# ============================================================================\n# EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    verdict, results = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T23:10:17.964283Z","iopub.execute_input":"2026-02-06T23:10:17.965031Z","iopub.status.idle":"2026-02-06T23:12:14.837805Z","shell.execute_reply.started":"2026-02-06T23:10:17.965003Z","shell.execute_reply":"2026-02-06T23:12:14.837106Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\n SWARM-MAMBA: PROOF OF CONCEPT VALIDATION\n Asynchronous Parallel State Machines with Emergent Consensus\n======================================================================\n\n Device: cuda\n Config: ExperimentConfig(seed=42, img_size=64, patch_size=8, n_classes=2, n_train=200, n_test=56, epochs=12, batch_size=32, lr=0.001, accuracy_threshold=0.65, relative_accuracy_threshold=0.9, speedup_threshold=1.0, consensus_threshold=0.6)\n\n======================================================================\n RUNNING TEST SUITE\n======================================================================\n\n[TEST 1] SelectiveSSM forward pass...\n  ‚úì Output shape and values correct\n\n[TEST 2] MicroMamba agent processing...\n  ‚úì Agent processes partition correctly\n\n[TEST 3] Inter-agent message passing...\n  ‚úì Message passing modifies states correctly\n\n[TEST 4] Emergent consensus...\n  ‚úì Consensus produces valid output (rate: 0.88)\n\n[TEST 5] SwarmMamba end-to-end...\n  ‚úì SwarmMamba produces valid predictions\n\n[TEST 6] Parameter efficiency check...\n  Swarm params: 335,279\n  Single params: 509,570\n  Ratio: 0.66x\n  ‚úì Parameter counts reasonable\n\n[TEST 7] Gradient flow through architecture...\n  ‚úì Gradients flow correctly\n\n======================================================================\n ALL TESTS PASSED ‚úì\n======================================================================\n\n--------------------------------------------------\n DATA GENERATION\n--------------------------------------------------\n Generating synthetic medical imaging dataset...\n  Training samples: 200\n  Test samples: 56\n  Image size: 64x64x3\n  Classes: 2 (benign/malignant)\n  Class balance: 100/100\n\n--------------------------------------------------\n EXPERIMENT 1: Single Mamba (Baseline)\n--------------------------------------------------\n  Parameters: 509,570\n  Training...\n    Epoch 4/12 - Loss: 0.0925\n    Epoch 8/12 - Loss: 0.0458\n    Epoch 12/12 - Loss: 0.0023\n  Evaluating...\n\n  Results:\n    Accuracy: 100.0%\n    Inference time: 38.10 ms/sample\n\n--------------------------------------------------\n EXPERIMENT 2: Swarm-Mamba (k=4 agents)\n--------------------------------------------------\n  Parameters: 335,279\n  Parameter ratio: 0.66x baseline\n  Training...\n    Epoch 4/12 - Loss: 0.3703\n    Epoch 8/12 - Loss: 0.0303\n    Epoch 12/12 - Loss: -0.0142\n  Evaluating...\n\n  Results:\n    Accuracy: 100.0%\n    Inference time: 75.36 ms/sample\n    Consensus rate: 100.0%\n\n--------------------------------------------------\n EXPERIMENT 3: Agent Scaling Analysis\n--------------------------------------------------\n\n  Testing k=2 agents...\n    k=2: Acc=100.0%, Time=64.63ms, Consensus=99.1%\n\n  Testing k=8 agents...\n    k=8: Acc=100.0%, Time=92.37ms, Consensus=99.3%\n\n======================================================================\n EVALUATION RESULTS & DECISION\n======================================================================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇCriterion                ‚îÇ  Achieved  ‚îÇ Threshold  ‚îÇ  Status  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇAbsolute Accuracy        ‚îÇ   100.0%   ‚îÇ    ‚â•65%    ‚îÇ  ‚úì PASS  ‚îÇ\n‚îÇRelative Accuracy        ‚îÇ   100.0%   ‚îÇ    ‚â•90%    ‚îÇ  ‚úì PASS  ‚îÇ\n‚îÇInference Speed          ‚îÇ   0.51x    ‚îÇ   ‚â•1.0x    ‚îÇ  ‚úó FAIL  ‚îÇ\n‚îÇAgent Consensus          ‚îÇ   100.0%   ‚îÇ    ‚â•60%    ‚îÇ  ‚úì PASS  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n Agent Scaling Analysis:\n    k=2: 100.0%\n    k=4: 100.0%\n    k=8: 100.0%\n\n======================================================================\n STRATEGIC VERDICT\n======================================================================\n\n  üü° FINAL VERDICT: IMPROVE\n  üìä Criteria Passed: 3/4\n  üìã Recommendation: REFINE ARCHITECTURE - ADDRESS IDENTIFIED GAPS\n\n--------------------------------------------------\n KEY INSIGHTS\n--------------------------------------------------\n  ‚úì Distributed processing successfully maintains accuracy\n  ‚úì High agent consensus indicates stable learned representations\n  ‚ö† Communication overhead exceeds parallelization gain - optimize message compression\n\n--------------------------------------------------\n TECHNICAL RECOMMENDATIONS FOR Q1\n--------------------------------------------------\n  1. Reduce message passing overhead via sparse communication\n  2. Implement asynchronous update (not all agents sync every round)\n  3. Add contrastive loss between agent representations\n  4. Experiment with hierarchical agent topology (tree vs. full)\n\n======================================================================\n Q1 RESEARCH DECISION: IMPROVE\n======================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# experiments ","metadata":{}},{"cell_type":"code","source":"\"\"\"\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSWARM-MAMBA: Q1 PUBLICATION CODE - PART 1 OF 2\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nOptimized for: Kaggle P100 GPU (16GB VRAM)\nContents:\n  - T1: Comparative Benchmarking (Swarm-Mamba vs Vim vs ViT) - PARAMETER MATCHED\n  - T2: Inter-Agent Consensus Analysis\n  \nHardware Profile:\n  - GPU: NVIDIA P100 (16GB)\n  - Batch Size: 200 (optimized for memory)\n  - Epochs: 5 (balanced speed/quality)\n  \nFAIR COMPARISON: All models ~520K parameters (within 1%)\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport gc\nimport warnings\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import (\n    roc_auc_score, f1_score, accuracy_score, \n    precision_score, recall_score, confusion_matrix,\n    cohen_kappa_score\n)\nfrom scipy import stats\n\nwarnings.filterwarnings('ignore')\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 1: CONFIGURATION (OPTIMIZED FOR P100 - ~520K PARAMS)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@dataclass\nclass Config:\n    seeds: List[int] = field(default_factory=lambda: [42, 123, 2024])\n    \n    data_root: str = \"/kaggle/input/breakhis-400x/BreaKHis 400X\"\n    img_size: int = 224\n    n_classes: int = 2\n    \n    batch_size: int = 200         \n    epochs: int = 5             \n    lr: float = 3e-4\n    weight_decay: float = 0.01\n    \n    # Swarm-Mamba configuration (target: ~524K params)\n    d_model: int = 80\n    n_layers: int = 2\n    d_state: int = 16\n    n_agents: int = 4\n    n_comm_rounds: int = 2\n    patch_size: int = 16\n    \n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    num_workers: int = 2\n    pin_memory: bool = True\n    \nCONFIG = Config()\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\ndef get_memory_stats():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**3\n        reserved = torch.cuda.memory_reserved() / 1024**3\n        max_allocated = torch.cuda.max_memory_allocated() / 1024**3\n        return {'allocated': allocated, 'reserved': reserved, 'max': max_allocated}\n    return {'allocated': 0, 'reserved': 0, 'max': 0}\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"‚ïê\" * 70)\nprint(\"SWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS - PART 1\")\nprint(\"‚ïê\" * 70)\nprint(f\"Device: {CONFIG.device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nprint(f\"Seeds: {CONFIG.seeds}\")\nprint(f\"Epochs: {CONFIG.epochs}\")\nprint(f\"Batch Size: {CONFIG.batch_size}\")\nprint(f\"Image Size: {CONFIG.img_size}x{CONFIG.img_size}\")\nprint(\"‚ïê\" * 70)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 2: DATASET\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass BreaKHisDataset(Dataset):\n    def __init__(self, root_dir: str, split: str = \"train\", transform=None):\n        self.root_dir = Path(root_dir)\n        self.split = split\n        self.transform = transform\n        self.samples = []\n        self.labels = []\n        self._load_data()\n        \n    def _load_data(self):\n        split_dir = self.root_dir / self.split\n        for class_idx, class_name in enumerate([\"benign\", \"malignant\"]):\n            class_dir = split_dir / class_name\n            if class_dir.exists():\n                for img_path in sorted(class_dir.glob(\"*.png\")):\n                    self.samples.append(str(img_path))\n                    self.labels.append(class_idx)\n        self.labels = np.array(self.labels)\n        \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.samples[idx]).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, self.labels[idx]\n\ndef get_transforms(is_train: bool = True):\n    if is_train:\n        return transforms.Compose([\n            transforms.Resize((CONFIG.img_size, CONFIG.img_size)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return transforms.Compose([\n        transforms.Resize((CONFIG.img_size, CONFIG.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 3: SELECTIVE STATE SPACE MODEL (OPTIMIZED)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass SelectiveSSM(nn.Module):\n    def __init__(self, d_model: int, d_state: int = 16, expand: int = 2):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_inner = d_model * expand\n        self.dt_rank = max(d_model // 16, 1)\n        \n        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, 4, padding=2, groups=self.d_inner)\n        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n        \n        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n        self.A_log = nn.Parameter(torch.log(A))\n        self.D = nn.Parameter(torch.ones(self.d_inner))\n        \n        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n        self.norm = nn.LayerNorm(self.d_inner)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, L, _ = x.shape\n        \n        xz = self.in_proj(x)\n        x_branch, z = xz.chunk(2, dim=-1)\n        \n        x_branch = x_branch.transpose(1, 2)\n        x_branch = self.conv1d(x_branch)[:, :, :L]\n        x_branch = x_branch.transpose(1, 2)\n        x_branch = F.silu(x_branch)\n        \n        x_dbl = self.x_proj(x_branch)\n        dt, B_param, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n        dt = F.softplus(self.dt_proj(dt))\n        \n        A = -torch.exp(self.A_log)\n        y = self._scan_optimized(x_branch, dt, A, B_param, C)\n        \n        y = self.norm(y)\n        y = y * F.silu(z)\n        return self.out_proj(y)\n    \n    def _scan_optimized(self, x, dt, A, B, C):\n        B_size, L, D = x.shape\n        \n        chunk_size = 32\n        outputs = []\n        h = torch.zeros(B_size, D, self.d_state, device=x.device, dtype=x.dtype)\n        \n        for start in range(0, L, chunk_size):\n            end = min(start + chunk_size, L)\n            chunk_out = []\n            \n            for t in range(start, end):\n                dt_t = dt[:, t, :].unsqueeze(-1)\n                dA = torch.exp(A.view(1, 1, -1) * dt_t)\n                dB = dt_t * B[:, t, :].unsqueeze(1)\n                h = dA * h + dB * x[:, t, :].unsqueeze(-1)\n                y_t = (h * C[:, t, :].unsqueeze(1)).sum(-1) + self.D * x[:, t, :]\n                chunk_out.append(y_t)\n            \n            outputs.extend(chunk_out)\n        \n        return torch.stack(outputs, dim=1)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 4: MICRO-MAMBA AGENT\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass MicroMamba(nn.Module):\n    def __init__(self, d_model: int, n_layers: int = 2, d_state: int = 16):\n        super().__init__()\n        self.d_model = d_model\n        self.blocks = nn.ModuleList([\n            nn.ModuleDict({\n                'norm': nn.LayerNorm(d_model),\n                'ssm': SelectiveSSM(d_model, d_state),\n                'drop': nn.Dropout(0.1)\n            }) for _ in range(n_layers)\n        ])\n        self.agent_embed = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = torch.cat([self.agent_embed.expand(B, -1, -1), x], dim=1)\n        \n        for block in self.blocks:\n            res = x\n            x = block['norm'](x)\n            x = block['ssm'](x)\n            x = block['drop'](x)\n            x = res + x\n            \n        return x[:, 1:, :]\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 5: MESSAGE PASSING & CONSENSUS\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass MessagePassing(nn.Module):\n    def __init__(self, d_model: int, n_agents: int, n_heads: int = 4):\n        super().__init__()\n        self.n_agents = n_agents\n        self.d_model = d_model\n        self.d_compress = d_model // 2\n        \n        self.compress = nn.Sequential(\n            nn.Linear(d_model, self.d_compress),\n            nn.GELU(),\n            nn.LayerNorm(self.d_compress)\n        )\n        self.decompress = nn.Linear(self.d_compress, d_model)\n        \n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=0.1, batch_first=True)\n        self.alpha = nn.Parameter(torch.tensor(0.3))\n        \n    def forward(self, states: List[torch.Tensor]) -> List[torch.Tensor]:\n        B = states[0].shape[0]\n        \n        summaries = torch.stack([self.compress(s.mean(dim=1)) for s in states], dim=1)\n        decompressed = self.decompress(summaries)\n        \n        attn_out, _ = self.attn(decompressed, decompressed, decompressed)\n        \n        alpha = torch.sigmoid(self.alpha)\n        updated = []\n        for i, s in enumerate(states):\n            msg = attn_out[:, i:i+1, :].expand(-1, s.shape[1], -1)\n            updated.append(s + alpha * msg)\n            \n        return updated\n\nclass Consensus(nn.Module):\n    def __init__(self, d_model: int, n_agents: int, n_classes: int):\n        super().__init__()\n        self.n_agents = n_agents\n        self.importance = nn.Parameter(torch.ones(n_agents) / n_agents)\n        \n        self.agent_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Linear(d_model, d_model // 2),\n                nn.GELU(),\n                nn.Dropout(0.1),\n                nn.Linear(d_model // 2, n_classes)\n            ) for _ in range(n_agents)\n        ])\n        \n        self.consensus_head = nn.Sequential(\n            nn.LayerNorm(d_model * n_agents),\n            nn.Linear(d_model * n_agents, d_model),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model, n_classes)\n        )\n        \n    def forward(self, states: List[torch.Tensor]) -> Tuple[torch.Tensor, List[torch.Tensor], Dict]:\n        pooled = [s.mean(dim=1) for s in states]\n        individual = [head(p) for head, p in zip(self.agent_heads, pooled)]\n        \n        concat = torch.cat(pooled, dim=-1)\n        consensus_logits = self.consensus_head(concat)\n        \n        with torch.no_grad():\n            preds = torch.stack([torch.argmax(l, dim=-1) for l in individual], dim=1)\n            mode_pred = torch.mode(preds, dim=1).values\n            agreement = (preds == mode_pred.unsqueeze(1)).float().mean()\n            \n            kappas = []\n            for i in range(len(individual)):\n                for j in range(i+1, len(individual)):\n                    p_i = torch.argmax(individual[i], dim=-1).cpu().numpy()\n                    p_j = torch.argmax(individual[j], dim=-1).cpu().numpy()\n                    if len(np.unique(p_i)) > 1 and len(np.unique(p_j)) > 1:\n                        try:\n                            kappas.append(cohen_kappa_score(p_i, p_j))\n                        except:\n                            pass\n            \n        metrics = {\n            'consensus_rate': agreement.item(),\n            'mean_kappa': np.mean(kappas) if kappas else 1.0\n        }\n        \n        return consensus_logits, individual, metrics\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 6: SWARM-MAMBA ARCHITECTURE\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass SwarmMamba(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 224,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        n_agents: int = 4,\n        d_model: int = 80,\n        n_layers: int = 2,\n        n_classes: int = 2,\n        n_comm_rounds: int = 2,\n        d_state: int = 16,\n        use_communication: bool = True\n    ):\n        super().__init__()\n        self.n_agents = n_agents\n        self.n_comm_rounds = n_comm_rounds\n        self.use_communication = use_communication\n        \n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.patch_embed = nn.Conv2d(in_chans, d_model, patch_size, patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, d_model) * 0.02)\n        \n        self.agents = nn.ModuleList([\n            MicroMamba(d_model, n_layers, d_state) for _ in range(n_agents)\n        ])\n        \n        self.message_passing = MessagePassing(d_model, n_agents)\n        self.consensus = Consensus(d_model, n_agents, n_classes)\n        \n    def partition(self, x: torch.Tensor) -> List[torch.Tensor]:\n        B, N, D = x.shape\n        per_agent = N // self.n_agents\n        return [x[:, i*per_agent:(i+1)*per_agent if i < self.n_agents-1 else N, :].clone() \n                for i in range(self.n_agents)]\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor], Dict]:\n        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n        x = x + self.pos_embed\n        \n        partitions = self.partition(x)\n        states = [agent(part) for agent, part in zip(self.agents, partitions)]\n        \n        for _ in range(self.n_comm_rounds):\n            if self.use_communication:\n                states = self.message_passing(states)\n            states = [agent(s) for agent, s in zip(self.agents, states)]\n        \n        return self.consensus(states)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 7: BASELINE MODELS (PARAMETER-MATCHED ~520K)\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass VisionMamba(nn.Module):\n    \"\"\"\n    Vision Mamba baseline - PARAMETER MATCHED to Swarm-Mamba (~522K)\n    Config: d_model=82, n_layers=9\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, d_model=82, \n                 n_layers=9, n_classes=2, d_state=16):\n        super().__init__()\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.patch_embed = nn.Conv2d(in_chans, d_model, patch_size, patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, d_model) * 0.02)\n        \n        self.blocks = nn.ModuleList([\n            nn.ModuleDict({\n                'norm': nn.LayerNorm(d_model),\n                'ssm': SelectiveSSM(d_model, d_state)\n            }) for _ in range(n_layers)\n        ])\n        \n        self.head = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, n_classes)\n        )\n        \n    def forward(self, x):\n        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n        x = x + self.pos_embed\n        \n        for block in self.blocks:\n            x = x + block['ssm'](block['norm'](x))\n        \n        return self.head(x.mean(dim=1))\n\nclass SimpleViT(nn.Module):\n    \"\"\"\n    Vision Transformer baseline - PARAMETER MATCHED to Swarm-Mamba (~521K)\n    Config: d_model=102, n_layers=5, mlp_ratio=2, n_heads=6\n    Note: 102 is divisible by 6 (102/6=17)\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, d_model=102,\n                 n_layers=5, n_heads=6, n_classes=2, mlp_ratio=2):\n        super().__init__()\n        self.n_patches = (img_size // patch_size) ** 2\n        \n        self.patch_embed = nn.Conv2d(in_chans, d_model, patch_size, patch_size)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, d_model) * 0.02)\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, \n            nhead=n_heads, \n            dim_feedforward=d_model * mlp_ratio,\n            dropout=0.1, \n            activation='gelu', \n            batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, n_layers)\n        \n        self.head = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, n_classes)\n        )\n        \n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n        cls = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls, x], dim=1)\n        x = x + self.pos_embed\n        x = self.encoder(x)\n        return self.head(x[:, 0])\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 8: PARAMETER VERIFICATION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef verify_parameter_matching():\n    \"\"\"Verify all models have approximately equal parameters (~520K, within 1%)\"\"\"\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"PARAMETER VERIFICATION FOR FAIR COMPARISON\")\n    print(\"Target: ~520K parameters per model (within 1%)\")\n    print(\"‚ïê\" * 70)\n    \n    models = {\n        'Swarm-Mamba': SwarmMamba(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            n_agents=CONFIG.n_agents,\n            d_model=CONFIG.d_model,       # 80\n            n_layers=CONFIG.n_layers,      # 2\n            n_classes=CONFIG.n_classes,\n            n_comm_rounds=CONFIG.n_comm_rounds,\n            d_state=CONFIG.d_state\n        ),\n        'Vision-Mamba': VisionMamba(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            d_model=82,       # Tuned for ~522K\n            n_layers=9,       # Tuned for ~522K\n            n_classes=CONFIG.n_classes,\n            d_state=CONFIG.d_state\n        ),\n        'ViT': SimpleViT(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            d_model=102,      # Tuned for ~521K\n            n_layers=5,       # Tuned for ~521K\n            n_heads=6,        # 102 / 6 = 17 (must divide evenly)\n            n_classes=CONFIG.n_classes,\n            mlp_ratio=2\n        )\n    }\n    \n    param_counts = {}\n    print(f\"\\n{'Model':<20} {'Parameters':>15} {'Relative':>12}\")\n    print(\"‚îÄ\" * 50)\n    \n    for name, model in models.items():\n        params = count_parameters(model)\n        param_counts[name] = params\n    \n    base_params = param_counts['Swarm-Mamba']\n    \n    for name, params in param_counts.items():\n        relative = params / base_params\n        print(f\"{name:<20} {params:>15,} {relative:>11.3f}x\")\n    \n    min_params = min(param_counts.values())\n    max_params = max(param_counts.values())\n    ratio = max_params / min_params\n    \n    print(\"‚îÄ\" * 50)\n    print(f\"Parameter Ratio (max/min): {ratio:.4f}x\")\n    print(f\"Target: < 1.05x for fair comparison\")\n    \n    if ratio <= 1.01:\n        print(\"‚úì EXCELLENT: All models within 1% parameter range\")\n    elif ratio <= 1.05:\n        print(\"‚úì FAIR COMPARISON: All models within 5% parameter range\")\n    elif ratio <= 1.10:\n        print(\"‚ö† ACCEPTABLE: All models within 10% parameter range\")\n    else:\n        print(\"‚úó WARNING: Parameter difference exceeds 10%\")\n    \n    return param_counts\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 9: TRAINING & EVALUATION WITH TIMING\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass Timer:\n    def __init__(self):\n        self.times = {}\n        \n    def start(self, name):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.times[name] = {'start': time.perf_counter()}\n        \n    def stop(self, name):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.times[name]['end'] = time.perf_counter()\n        self.times[name]['duration'] = self.times[name]['end'] - self.times[name]['start']\n        return self.times[name]['duration']\n    \n    def get(self, name):\n        return self.times.get(name, {}).get('duration', 0)\n\ndef train_epoch(model, loader, optimizer, criterion, device, is_swarm=False):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_x, batch_y in loader:\n        batch_x, batch_y = batch_x.to(device, non_blocking=True), batch_y.to(device, non_blocking=True)\n        \n        optimizer.zero_grad(set_to_none=True)\n        \n        if is_swarm:\n            logits, individual, _ = model(batch_x)\n            loss = criterion(logits, batch_y)\n            for ind in individual:\n                loss += 0.1 * criterion(ind, batch_y)\n        else:\n            logits = model(batch_x)\n            loss = criterion(logits, batch_y)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        total_loss += loss.item() * batch_x.size(0)\n        preds = torch.argmax(logits, dim=-1)\n        correct += (preds == batch_y).sum().item()\n        total += batch_y.size(0)\n    \n    return total_loss / total, correct / total\n\n@torch.no_grad()\ndef evaluate(model, loader, device, is_swarm=False):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    consensus_data = {'rates': [], 'kappas': []}\n    \n    for batch_x, batch_y in loader:\n        batch_x, batch_y = batch_x.to(device, non_blocking=True), batch_y.to(device, non_blocking=True)\n        \n        if is_swarm:\n            logits, individual, metrics = model(batch_x)\n            consensus_data['rates'].append(metrics['consensus_rate'])\n            consensus_data['kappas'].append(metrics['mean_kappa'])\n        else:\n            logits = model(batch_x)\n        \n        probs = F.softmax(logits, dim=-1)\n        preds = torch.argmax(logits, dim=-1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch_y.cpu().numpy())\n        all_probs.extend(probs[:, 1].cpu().numpy())\n    \n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    \n    results = {\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'auc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.5,\n        'f1': f1_score(all_labels, all_preds, average='weighted'),\n        'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n        'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n        'sensitivity': recall_score(all_labels, all_preds, pos_label=1, zero_division=0),\n        'specificity': recall_score(all_labels, all_preds, pos_label=0, zero_division=0)\n    }\n    \n    if consensus_data['rates']:\n        results['consensus_rate'] = np.mean(consensus_data['rates'])\n        results['mean_kappa'] = np.mean(consensus_data['kappas'])\n    \n    return results\n\n@torch.no_grad()\ndef measure_inference_time(model, loader, device, is_swarm=False, n_warmup=5, n_runs=50):\n    model.eval()\n    \n    batch_x, _ = next(iter(loader))\n    batch_x = batch_x.to(device)\n    \n    for _ in range(n_warmup):\n        if is_swarm:\n            _ = model(batch_x)\n        else:\n            _ = model(batch_x)\n    \n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    \n    times = []\n    for _ in range(n_runs):\n        start = time.perf_counter()\n        if is_swarm:\n            _ = model(batch_x)\n        else:\n            _ = model(batch_x)\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        times.append(time.perf_counter() - start)\n    \n    return {\n        'mean_ms': np.mean(times) * 1000,\n        'std_ms': np.std(times) * 1000,\n        'per_sample_ms': np.mean(times) * 1000 / batch_x.size(0),\n        'throughput': batch_x.size(0) / np.mean(times)\n    }\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 10: EXPERIMENT T1 - COMPARATIVE BENCHMARKING\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef run_T1_benchmarking():\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"EXPERIMENT T1: ACCURACY VS COMPLEXITY BENCHMARKING\")\n    print(\"(PARAMETER-MATCHED COMPARISON ~520K params each)\")\n    print(\"‚ïê\" * 70)\n    \n    timer = Timer()\n    timer.start('T1_total')\n    \n    train_ds = BreaKHisDataset(CONFIG.data_root, \"train\", get_transforms(True))\n    test_ds = BreaKHisDataset(CONFIG.data_root, \"test\", get_transforms(False))\n    \n    print(f\"\\nDataset Statistics:\")\n    print(f\"  Training samples: {len(train_ds)}\")\n    print(f\"  Test samples: {len(test_ds)}\")\n    print(f\"  Train class distribution: Benign={np.sum(train_ds.labels==0)}, Malignant={np.sum(train_ds.labels==1)}\")\n    print(f\"  Test class distribution: Benign={np.sum(test_ds.labels==0)}, Malignant={np.sum(test_ds.labels==1)}\")\n    \n    # Parameter-matched model configurations (~520K each)\n    models_config = {\n        'Swarm-Mamba': lambda: SwarmMamba(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            n_agents=CONFIG.n_agents,\n            d_model=CONFIG.d_model,        # 80\n            n_layers=CONFIG.n_layers,       # 2\n            n_classes=CONFIG.n_classes,\n            n_comm_rounds=CONFIG.n_comm_rounds,\n            d_state=CONFIG.d_state\n        ),\n        'Vision-Mamba': lambda: VisionMamba(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            d_model=82,        # Tuned for ~522K\n            n_layers=9,        # Tuned for ~522K\n            n_classes=CONFIG.n_classes,\n            d_state=CONFIG.d_state\n        ),\n        'ViT': lambda: SimpleViT(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            d_model=102,       # Tuned for ~521K\n            n_layers=5,        # Tuned for ~521K\n            n_heads=6,         # 102/6=17\n            n_classes=CONFIG.n_classes,\n            mlp_ratio=2\n        )\n    }\n    \n    all_results = {name: {\n        'auc': [], 'acc': [], 'f1': [], 'precision': [], 'recall': [],\n        'sensitivity': [], 'specificity': [],\n        'params': 0, 'train_time': [], 'inference_time': []\n    } for name in models_config}\n    \n    # Verify and display parameter counts\n    print(\"\\n\" + \"‚îÄ\" * 50)\n    print(\"MODEL PARAMETER COUNTS (MATCHED ~520K)\")\n    print(\"‚îÄ\" * 50)\n    for name, model_fn in models_config.items():\n        model = model_fn()\n        params = count_parameters(model)\n        all_results[name]['params'] = params\n        print(f\"  {name:<15}: {params:>10,} parameters\")\n        del model\n    \n    params_list = [all_results[name]['params'] for name in models_config]\n    ratio = max(params_list) / min(params_list)\n    print(\"‚îÄ\" * 50)\n    print(f\"  Max/Min Ratio: {ratio:.4f}x {'‚úì' if ratio <= 1.05 else '‚ö†'}\")\n    print(\"‚îÄ\" * 50)\n    \n    for seed_idx, seed in enumerate(CONFIG.seeds):\n        print(f\"\\n{'‚îÄ' * 70}\")\n        print(f\"SEED {seed_idx+1}/{len(CONFIG.seeds)}: {seed}\")\n        print('‚îÄ' * 70)\n        set_seed(seed)\n        \n        train_loader = DataLoader(\n            train_ds, batch_size=CONFIG.batch_size, shuffle=True,\n            num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory\n        )\n        test_loader = DataLoader(\n            test_ds, batch_size=CONFIG.batch_size, shuffle=False,\n            num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory\n        )\n        \n        for model_name, model_fn in models_config.items():\n            print(f\"\\n  Model: {model_name}\")\n            print(f\"  {'-' * 40}\")\n            \n            clear_memory()\n            \n            model = model_fn().to(CONFIG.device)\n            params = count_parameters(model)\n            \n            print(f\"  Parameters: {params:,}\")\n            \n            is_swarm = 'Swarm' in model_name\n            optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.lr, weight_decay=CONFIG.weight_decay)\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, CONFIG.epochs)\n            criterion = nn.CrossEntropyLoss()\n            \n            timer.start(f'{model_name}_train')\n            \n            for epoch in range(CONFIG.epochs):\n                epoch_start = time.perf_counter()\n                train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, CONFIG.device, is_swarm)\n                scheduler.step()\n                epoch_time = time.perf_counter() - epoch_start\n                \n                print(f\"  Epoch {epoch+1:2d}/{CONFIG.epochs} | Loss: {train_loss:.4f} | \"\n                      f\"Acc: {train_acc:.4f} | Time: {epoch_time:.1f}s\")\n            \n            train_time = timer.stop(f'{model_name}_train')\n            all_results[model_name]['train_time'].append(train_time)\n            \n            results = evaluate(model, test_loader, CONFIG.device, is_swarm)\n            \n            inf_time = measure_inference_time(model, test_loader, CONFIG.device, is_swarm)\n            all_results[model_name]['inference_time'].append(inf_time['per_sample_ms'])\n            \n            for key in ['auc', 'acc', 'f1', 'precision', 'recall', 'sensitivity', 'specificity']:\n                if key == 'acc':\n                    all_results[model_name][key].append(results['accuracy'])\n                else:\n                    all_results[model_name][key].append(results.get(key, 0))\n            \n            mem = get_memory_stats()\n            print(f\"\\n  Test Results:\")\n            print(f\"    AUC: {results['auc']:.4f}\")\n            print(f\"    Accuracy: {results['accuracy']:.4f}\")\n            print(f\"    F1: {results['f1']:.4f}\")\n            print(f\"    Sensitivity: {results['sensitivity']:.4f}\")\n            print(f\"    Specificity: {results['specificity']:.4f}\")\n            print(f\"    Train Time: {train_time:.1f}s\")\n            print(f\"    Inference: {inf_time['per_sample_ms']:.2f} ms/sample\")\n            print(f\"    Throughput: {inf_time['throughput']:.1f} samples/s\")\n            print(f\"    GPU Memory: {mem['max']:.2f} GB\")\n            \n            del model\n            clear_memory()\n    \n    t1_time = timer.stop('T1_total')\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"T1 RESULTS SUMMARY (PARAMETER-MATCHED COMPARISON ~520K)\")\n    print(\"‚ïê\" * 70)\n    \n    print(f\"\\n{'Model':<15} {'Params':>10} {'AUC':>15} {'Accuracy':>15} {'F1':>15}\")\n    print(\"‚îÄ\" * 70)\n    \n    for name, data in all_results.items():\n        auc_mean, auc_std = np.mean(data['auc']), np.std(data['auc'])\n        acc_mean, acc_std = np.mean(data['acc']), np.std(data['acc'])\n        f1_mean, f1_std = np.mean(data['f1']), np.std(data['f1'])\n        \n        print(f\"{name:<15} {data['params']:>10,} \"\n              f\"{auc_mean:.4f}¬±{auc_std:.4f} \"\n              f\"{acc_mean:.4f}¬±{acc_std:.4f} \"\n              f\"{f1_mean:.4f}¬±{f1_std:.4f}\")\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"TIMING COMPARISON\")\n    print(\"‚îÄ\" * 70)\n    print(f\"\\n{'Model':<15} {'Train Time (s)':>18} {'Inference (ms/sample)':>25}\")\n    print(\"‚îÄ\" * 60)\n    \n    for name, data in all_results.items():\n        train_mean = np.mean(data['train_time'])\n        train_std = np.std(data['train_time'])\n        inf_mean = np.mean(data['inference_time'])\n        inf_std = np.std(data['inference_time'])\n        print(f\"{name:<15} {train_mean:>8.1f}¬±{train_std:>5.1f}      {inf_mean:>10.2f}¬±{inf_std:>5.2f}\")\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"STATISTICAL SIGNIFICANCE (Paired t-test: Swarm-Mamba vs Others)\")\n    print(\"‚îÄ\" * 70)\n    \n    swarm_auc = all_results['Swarm-Mamba']['auc']\n    for name in ['Vision-Mamba', 'ViT']:\n        other_auc = all_results[name]['auc']\n        if len(swarm_auc) >= 3 and len(other_auc) >= 3:\n            t_stat, p_val = stats.ttest_rel(swarm_auc, other_auc)\n            diff = np.mean(swarm_auc) - np.mean(other_auc)\n            effect = diff / np.std(other_auc) if np.std(other_auc) > 0 else 0\n            sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n            print(f\"  vs {name:<15}: Œî={diff:+.4f}, t={t_stat:>7.3f}, p={p_val:.4f} {sig}, Cohen's d={effect:.3f}\")\n    \n    print(f\"\\nT1 Total Execution Time: {t1_time/60:.2f} minutes\")\n    \n    return all_results\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 11: EXPERIMENT T2 - CONSENSUS ANALYSIS\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef run_T2_consensus_analysis():\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"EXPERIMENT T2: INTER-AGENT CONSENSUS ANALYSIS\")\n    print(\"‚ïê\" * 70)\n    \n    timer = Timer()\n    timer.start('T2_total')\n    \n    consensus_results = {\n        'overall_rate': [],\n        'overall_kappa': [],\n        'correct_consensus': [],\n        'incorrect_consensus': [],\n        'agent_accuracies': []\n    }\n    \n    for seed_idx, seed in enumerate(CONFIG.seeds):\n        print(f\"\\n{'‚îÄ' * 50}\")\n        print(f\"SEED {seed_idx+1}/{len(CONFIG.seeds)}: {seed}\")\n        print('‚îÄ' * 50)\n        set_seed(seed)\n        clear_memory()\n        \n        train_ds = BreaKHisDataset(CONFIG.data_root, \"train\", get_transforms(True))\n        test_ds = BreaKHisDataset(CONFIG.data_root, \"test\", get_transforms(False))\n        \n        train_loader = DataLoader(train_ds, batch_size=CONFIG.batch_size, shuffle=True,\n                                 num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n        test_loader = DataLoader(test_ds, batch_size=CONFIG.batch_size, shuffle=False,\n                                num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n        \n        model = SwarmMamba(\n            img_size=CONFIG.img_size,\n            patch_size=CONFIG.patch_size,\n            n_agents=CONFIG.n_agents,\n            d_model=CONFIG.d_model,\n            n_layers=CONFIG.n_layers,\n            n_classes=CONFIG.n_classes,\n            n_comm_rounds=CONFIG.n_comm_rounds\n        ).to(CONFIG.device)\n        \n        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.lr, weight_decay=CONFIG.weight_decay)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, CONFIG.epochs)\n        criterion = nn.CrossEntropyLoss()\n        \n        print(\"  Training Swarm-Mamba...\")\n        for epoch in range(CONFIG.epochs):\n            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, CONFIG.device, is_swarm=True)\n            scheduler.step()\n            print(f\"    Epoch {epoch+1:2d}/{CONFIG.epochs} | Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n        \n        print(\"  Analyzing consensus patterns...\")\n        model.eval()\n        \n        batch_rates = []\n        batch_kappas = []\n        correct_rates = []\n        incorrect_rates = []\n        agent_preds_all = [[] for _ in range(CONFIG.n_agents)]\n        all_labels = []\n        \n        with torch.no_grad():\n            for batch_x, batch_y in test_loader:\n                batch_x, batch_y = batch_x.to(CONFIG.device), batch_y.to(CONFIG.device)\n                logits, individual, metrics = model(batch_x)\n                \n                batch_rates.append(metrics['consensus_rate'])\n                batch_kappas.append(metrics['mean_kappa'])\n                \n                preds = torch.argmax(logits, dim=-1)\n                correct_mask = (preds == batch_y)\n                \n                ind_preds = torch.stack([torch.argmax(l, dim=-1) for l in individual], dim=1)\n                \n                for i in range(len(batch_y)):\n                    sample_preds = ind_preds[i]\n                    mode_pred = torch.mode(sample_preds).values\n                    sample_consensus = (sample_preds == mode_pred).float().mean().item()\n                    \n                    if correct_mask[i]:\n                        correct_rates.append(sample_consensus)\n                    else:\n                        incorrect_rates.append(sample_consensus)\n                \n                for a in range(CONFIG.n_agents):\n                    agent_preds_all[a].extend(torch.argmax(individual[a], dim=-1).cpu().numpy())\n                all_labels.extend(batch_y.cpu().numpy())\n        \n        agent_accs = []\n        all_labels = np.array(all_labels)\n        for a in range(CONFIG.n_agents):\n            agent_preds = np.array(agent_preds_all[a])\n            agent_acc = accuracy_score(all_labels, agent_preds)\n            agent_accs.append(agent_acc)\n        \n        consensus_results['overall_rate'].append(np.mean(batch_rates))\n        consensus_results['overall_kappa'].append(np.mean(batch_kappas))\n        consensus_results['correct_consensus'].append(np.mean(correct_rates) if correct_rates else 0)\n        consensus_results['incorrect_consensus'].append(np.mean(incorrect_rates) if incorrect_rates else 0)\n        consensus_results['agent_accuracies'].append(agent_accs)\n        \n        print(f\"\\n  Seed {seed} Results:\")\n        print(f\"    Overall Consensus Rate: {np.mean(batch_rates):.4f}\")\n        print(f\"    Mean Inter-Agent Kappa: {np.mean(batch_kappas):.4f}\")\n        print(f\"    Consensus on Correct: {np.mean(correct_rates):.4f}\" if correct_rates else \"    Consensus on Correct: N/A\")\n        print(f\"    Consensus on Incorrect: {np.mean(incorrect_rates):.4f}\" if incorrect_rates else \"    Consensus on Incorrect: N/A\")\n        print(f\"    Agent Accuracies: {[f'{a:.4f}' for a in agent_accs]}\")\n        \n        del model\n        clear_memory()\n    \n    t2_time = timer.stop('T2_total')\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"T2 RESULTS SUMMARY\")\n    print(\"‚ïê\" * 70)\n    \n    print(f\"\\nOverall Consensus Rate: {np.mean(consensus_results['overall_rate']):.4f} ¬± \"\n          f\"{np.std(consensus_results['overall_rate']):.4f}\")\n    print(f\"Mean Inter-Agent Kappa: {np.mean(consensus_results['overall_kappa']):.4f} ¬± \"\n          f\"{np.std(consensus_results['overall_kappa']):.4f}\")\n    \n    print(f\"\\nConsensus on CORRECT predictions: {np.mean(consensus_results['correct_consensus']):.4f} ¬± \"\n          f\"{np.std(consensus_results['correct_consensus']):.4f}\")\n    print(f\"Consensus on INCORRECT predictions: {np.mean(consensus_results['incorrect_consensus']):.4f} ¬± \"\n          f\"{np.std(consensus_results['incorrect_consensus']):.4f}\")\n    \n    threshold_met = np.mean(consensus_results['correct_consensus']) > 0.85\n    print(f\"\\nT2 Success Criterion (Consensus > 85% on correct): {'PASSED ‚úì' if threshold_met else 'NOT MET ‚úó'}\")\n    \n    if len(consensus_results['correct_consensus']) >= 2 and len(consensus_results['incorrect_consensus']) >= 2:\n        t_stat, p_val = stats.ttest_ind(\n            consensus_results['correct_consensus'],\n            consensus_results['incorrect_consensus']\n        )\n        print(f\"\\nConsensus Difference Test:\")\n        print(f\"  t-statistic: {t_stat:.4f}\")\n        print(f\"  p-value: {p_val:.4f}\")\n        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n        print(f\"  Significance: {sig if sig else 'Not significant'}\")\n    \n    print(\"\\n\" + \"‚îÄ\" * 50)\n    print(\"AGENT SPECIALIZATION ANALYSIS\")\n    print(\"‚îÄ\" * 50)\n    \n    mean_agent_accs = np.mean(consensus_results['agent_accuracies'], axis=0)\n    std_agent_accs = np.std(consensus_results['agent_accuracies'], axis=0)\n    \n    for a in range(CONFIG.n_agents):\n        print(f\"  Agent {a+1} Accuracy: {mean_agent_accs[a]:.4f} ¬± {std_agent_accs[a]:.4f}\")\n    \n    diversity = np.std(mean_agent_accs)\n    print(f\"\\n  Agent Diversity (œÉ of accuracies): {diversity:.4f}\")\n    print(f\"  Interpretation: {'Low diversity - agents similar' if diversity < 0.02 else 'Moderate diversity' if diversity < 0.05 else 'High diversity - agents specialized'}\")\n    \n    print(f\"\\nT2 Total Execution Time: {t2_time/60:.2f} minutes\")\n    \n    return consensus_results\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION 12: MAIN EXECUTION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"‚ñà\" * 70)\n    print(\"‚ñà\" + \" \" * 68 + \"‚ñà\")\n    print(\"‚ñà\" + \"   SWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS\".center(68) + \"‚ñà\")\n    print(\"‚ñà\" + \"   PART 1: Benchmarking & Consensus Analysis\".center(68) + \"‚ñà\")\n    print(\"‚ñà\" + \"   (PARAMETER-MATCHED FAIR COMPARISON ~520K)\".center(68) + \"‚ñà\")\n    print(\"‚ñà\" + \" \" * 68 + \"‚ñà\")\n    print(\"‚ñà\" * 70)\n    \n    total_timer = Timer()\n    total_timer.start('total')\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"SYSTEM INFORMATION\")\n    print(\"‚îÄ\" * 70)\n    print(f\"PyTorch Version: {torch.__version__}\")\n    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"CUDA Version: {torch.version.cuda}\")\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    \n    data_path = Path(CONFIG.data_root)\n    if not data_path.exists():\n        print(f\"\\nERROR: Data not found at {CONFIG.data_root}\")\n        print(\"Please ensure BreaKHis 400X dataset is available.\")\n        sys.exit(1)\n    \n    # Verify parameter matching before experiments\n    param_counts = verify_parameter_matching()\n    \n    print(\"\\n\" + \"‚ñà\" * 70)\n    t1_results = run_T1_benchmarking()\n    \n    print(\"\\n\" + \"‚ñà\" * 70)\n    t2_results = run_T2_consensus_analysis()\n    \n    total_time = total_timer.stop('total')\n    final_mem = get_memory_stats()\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"PART 1 EXECUTION COMPLETE\")\n    print(\"‚ïê\" * 70)\n    print(f\"\\nTotal Execution Time: {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n    print(f\"Peak GPU Memory Used: {final_mem['max']:.2f} GB\")\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"EXPERIMENTS COMPLETED IN PART 1:\")\n    print(\"‚îÄ\" * 70)\n    print(\"  ‚úì Parameter Verification (all models ~520K, within 1%)\")\n    print(\"  ‚úì T1: Comparative Benchmarking (3 models √ó 3 seeds)\")\n    print(\"  ‚úì T2: Inter-Agent Consensus Analysis (3 seeds)\")\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"PROCEED TO PART 2 FOR:\")\n    print(\"‚îÄ\" * 70)\n    print(\"  ‚Üí T3: Noise Robustness Testing\")\n    print(\"  ‚Üí T4: Communication Ablation Study\")\n    print(\"  ‚Üí T5: Inference Latency Benchmarking\")\n    print(\"  ‚Üí Final Statistical Summary\")\n    print(\"  ‚Üí Publication-Ready Tables\")\n    print(\"‚ïê\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T00:10:02.272219Z","iopub.execute_input":"2026-02-07T00:10:02.272755Z","iopub.status.idle":"2026-02-07T00:29:38.323788Z","shell.execute_reply.started":"2026-02-07T00:10:02.272695Z","shell.execute_reply":"2026-02-07T00:29:38.323093Z"}},"outputs":[{"name":"stdout","text":"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS - PART 1\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nDevice: cuda\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 15.9 GB\nSeeds: [42, 123, 2024]\nEpochs: 5\nBatch Size: 200\nImage Size: 224x224\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n‚ñà                                                                    ‚ñà\n‚ñà                SWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS             ‚ñà\n‚ñà               PART 1: Benchmarking & Consensus Analysis            ‚ñà\n‚ñà               (PARAMETER-MATCHED FAIR COMPARISON ~520K)            ‚ñà\n‚ñà                                                                    ‚ñà\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSYSTEM INFORMATION\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nPyTorch Version: 2.8.0+cu126\nCUDA Available: True\nCUDA Version: 12.6\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 15.9 GB\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPARAMETER VERIFICATION FOR FAIR COMPARISON\nTarget: ~520K parameters per model (within 1%)\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nModel                     Parameters     Relative\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba                  524,343       1.000x\nVision-Mamba                 522,773       0.997x\nViT                          520,814       0.993x\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nParameter Ratio (max/min): 1.0068x\nTarget: < 1.05x for fair comparison\n‚úì EXCELLENT: All models within 1% parameter range\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nEXPERIMENT T1: ACCURACY VS COMPLEXITY BENCHMARKING\n(PARAMETER-MATCHED COMPARISON ~520K params each)\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nDataset Statistics:\n  Training samples: 1148\n  Test samples: 545\n  Train class distribution: Benign=371, Malignant=777\n  Test class distribution: Benign=176, Malignant=369\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nMODEL PARAMETER COUNTS (MATCHED ~520K)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Swarm-Mamba    :    524,343 parameters\n  Vision-Mamba   :    522,773 parameters\n  ViT            :    520,814 parameters\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Max/Min Ratio: 1.0068x ‚úì\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSEED 1/3: 42\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Model: Swarm-Mamba\n  ----------------------------------------\n  Parameters: 524,343\n  Epoch  1/5 | Loss: 0.8027 | Acc: 0.7239 | Time: 19.5s\n  Epoch  2/5 | Loss: 0.7280 | Acc: 0.7674 | Time: 14.1s\n  Epoch  3/5 | Loss: 0.6528 | Acc: 0.8188 | Time: 15.0s\n  Epoch  4/5 | Loss: 0.6109 | Acc: 0.8441 | Time: 14.6s\n  Epoch  5/5 | Loss: 0.5964 | Acc: 0.8449 | Time: 14.5s\n\n  Test Results:\n    AUC: 0.8091\n    Accuracy: 0.8404\n    F1: 0.8324\n    Sensitivity: 0.9485\n    Specificity: 0.6136\n    Train Time: 77.7s\n    Inference: 1.13 ms/sample\n    Throughput: 887.2 samples/s\n    GPU Memory: 9.07 GB\n\n  Model: Vision-Mamba\n  ----------------------------------------\n  Parameters: 522,773\n  Epoch  1/5 | Loss: 0.6149 | Acc: 0.6211 | Time: 25.2s\n  Epoch  2/5 | Loss: 0.5088 | Acc: 0.7770 | Time: 22.5s\n  Epoch  3/5 | Loss: 0.4531 | Acc: 0.8249 | Time: 21.9s\n  Epoch  4/5 | Loss: 0.4118 | Acc: 0.8476 | Time: 22.3s\n  Epoch  5/5 | Loss: 0.4150 | Acc: 0.8371 | Time: 22.2s\n\n  Test Results:\n    AUC: 0.8015\n    Accuracy: 0.8459\n    F1: 0.8396\n    Sensitivity: 0.9431\n    Specificity: 0.6420\n    Train Time: 114.1s\n    Inference: 1.48 ms/sample\n    Throughput: 675.8 samples/s\n    GPU Memory: 13.21 GB\n\n  Model: ViT\n  ----------------------------------------\n  Parameters: 520,814\n  Epoch  1/5 | Loss: 0.6396 | Acc: 0.6455 | Time: 11.3s\n  Epoch  2/5 | Loss: 0.5053 | Acc: 0.7892 | Time: 10.8s\n  Epoch  3/5 | Loss: 0.4603 | Acc: 0.8240 | Time: 10.6s\n  Epoch  4/5 | Loss: 0.4327 | Acc: 0.8345 | Time: 10.8s\n  Epoch  5/5 | Loss: 0.4216 | Acc: 0.8432 | Time: 10.8s\n\n  Test Results:\n    AUC: 0.7837\n    Accuracy: 0.8367\n    F1: 0.8302\n    Sensitivity: 0.9350\n    Specificity: 0.6307\n    Train Time: 54.4s\n    Inference: 0.21 ms/sample\n    Throughput: 4778.0 samples/s\n    GPU Memory: 1.25 GB\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSEED 2/3: 123\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Model: Swarm-Mamba\n  ----------------------------------------\n  Parameters: 524,343\n  Epoch  1/5 | Loss: 0.8467 | Acc: 0.7221 | Time: 14.8s\n  Epoch  2/5 | Loss: 0.6998 | Acc: 0.7927 | Time: 14.3s\n  Epoch  3/5 | Loss: 0.6626 | Acc: 0.8214 | Time: 14.9s\n  Epoch  4/5 | Loss: 0.6392 | Acc: 0.8197 | Time: 14.3s\n  Epoch  5/5 | Loss: 0.6280 | Acc: 0.8275 | Time: 14.8s\n\n  Test Results:\n    AUC: 0.8081\n    Accuracy: 0.8330\n    F1: 0.8214\n    Sensitivity: 0.9621\n    Specificity: 0.5625\n    Train Time: 73.0s\n    Inference: 1.16 ms/sample\n    Throughput: 859.6 samples/s\n    GPU Memory: 9.07 GB\n\n  Model: Vision-Mamba\n  ----------------------------------------\n  Parameters: 522,773\n  Epoch  1/5 | Loss: 0.6283 | Acc: 0.6080 | Time: 22.4s\n  Epoch  2/5 | Loss: 0.4998 | Acc: 0.8084 | Time: 22.4s\n  Epoch  3/5 | Loss: 0.4499 | Acc: 0.8293 | Time: 22.1s\n  Epoch  4/5 | Loss: 0.4422 | Acc: 0.8240 | Time: 22.5s\n  Epoch  5/5 | Loss: 0.4120 | Acc: 0.8406 | Time: 22.0s\n\n  Test Results:\n    AUC: 0.8055\n    Accuracy: 0.8440\n    F1: 0.8367\n    Sensitivity: 0.9485\n    Specificity: 0.6250\n    Train Time: 111.5s\n    Inference: 1.51 ms/sample\n    Throughput: 662.8 samples/s\n    GPU Memory: 13.21 GB\n\n  Model: ViT\n  ----------------------------------------\n  Parameters: 520,814\n  Epoch  1/5 | Loss: 0.6566 | Acc: 0.6847 | Time: 10.7s\n  Epoch  2/5 | Loss: 0.5330 | Acc: 0.7230 | Time: 10.6s\n  Epoch  3/5 | Loss: 0.4630 | Acc: 0.8136 | Time: 10.8s\n  Epoch  4/5 | Loss: 0.4360 | Acc: 0.8258 | Time: 10.6s\n  Epoch  5/5 | Loss: 0.4273 | Acc: 0.8389 | Time: 10.6s\n\n  Test Results:\n    AUC: 0.7819\n    Accuracy: 0.8239\n    F1: 0.8183\n    Sensitivity: 0.9160\n    Specificity: 0.6307\n    Train Time: 53.3s\n    Inference: 0.21 ms/sample\n    Throughput: 4776.0 samples/s\n    GPU Memory: 1.25 GB\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSEED 3/3: 2024\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Model: Swarm-Mamba\n  ----------------------------------------\n  Parameters: 524,343\n  Epoch  1/5 | Loss: 0.8381 | Acc: 0.7300 | Time: 14.6s\n  Epoch  2/5 | Loss: 0.7126 | Acc: 0.7892 | Time: 14.9s\n  Epoch  3/5 | Loss: 0.6495 | Acc: 0.8162 | Time: 15.1s\n  Epoch  4/5 | Loss: 0.5983 | Acc: 0.8415 | Time: 14.8s\n  Epoch  5/5 | Loss: 0.5794 | Acc: 0.8545 | Time: 14.5s\n\n  Test Results:\n    AUC: 0.8214\n    Accuracy: 0.8440\n    F1: 0.8363\n    Sensitivity: 0.9512\n    Specificity: 0.6193\n    Train Time: 73.8s\n    Inference: 1.16 ms/sample\n    Throughput: 865.5 samples/s\n    GPU Memory: 9.07 GB\n\n  Model: Vision-Mamba\n  ----------------------------------------\n  Parameters: 522,773\n  Epoch  1/5 | Loss: 0.5500 | Acc: 0.7395 | Time: 22.6s\n  Epoch  2/5 | Loss: 0.4479 | Acc: 0.8328 | Time: 22.5s\n  Epoch  3/5 | Loss: 0.4236 | Acc: 0.8423 | Time: 22.4s\n  Epoch  4/5 | Loss: 0.4074 | Acc: 0.8502 | Time: 22.3s\n  Epoch  5/5 | Loss: 0.3982 | Acc: 0.8537 | Time: 22.4s\n\n  Test Results:\n    AUC: 0.8020\n    Accuracy: 0.8550\n    F1: 0.8493\n    Sensitivity: 0.9485\n    Specificity: 0.6591\n    Train Time: 112.0s\n    Inference: 1.57 ms/sample\n    Throughput: 635.4 samples/s\n    GPU Memory: 13.21 GB\n\n  Model: ViT\n  ----------------------------------------\n  Parameters: 520,814\n  Epoch  1/5 | Loss: 0.6872 | Acc: 0.6141 | Time: 11.5s\n  Epoch  2/5 | Loss: 0.5800 | Acc: 0.6803 | Time: 10.9s\n  Epoch  3/5 | Loss: 0.5139 | Acc: 0.7787 | Time: 10.9s\n  Epoch  4/5 | Loss: 0.4668 | Acc: 0.8223 | Time: 11.1s\n  Epoch  5/5 | Loss: 0.4484 | Acc: 0.8232 | Time: 11.3s\n\n  Test Results:\n    AUC: 0.7840\n    Accuracy: 0.8147\n    F1: 0.8095\n    Sensitivity: 0.9051\n    Specificity: 0.6250\n    Train Time: 55.7s\n    Inference: 0.21 ms/sample\n    Throughput: 4778.1 samples/s\n    GPU Memory: 1.25 GB\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nT1 RESULTS SUMMARY (PARAMETER-MATCHED COMPARISON ~520K)\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nModel               Params             AUC        Accuracy              F1\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba        524,343 0.8128¬±0.0061 0.8391¬±0.0046 0.8300¬±0.0063\nVision-Mamba       522,773 0.8030¬±0.0018 0.8483¬±0.0048 0.8419¬±0.0054\nViT                520,814 0.7832¬±0.0009 0.8251¬±0.0090 0.8193¬±0.0085\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTIMING COMPARISON\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nModel               Train Time (s)     Inference (ms/sample)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba         74.8¬±  2.1            1.15¬± 0.02\nVision-Mamba       112.5¬±  1.1            1.52¬± 0.04\nViT                 54.4¬±  1.0            0.21¬± 0.00\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSTATISTICAL SIGNIFICANCE (Paired t-test: Swarm-Mamba vs Others)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  vs Vision-Mamba   : Œî=+0.0098, t=  1.977, p=0.1867 , Cohen's d=5.574\n  vs ViT            : Œî=+0.0297, t=  7.635, p=0.0167 *, Cohen's d=31.936\n\nT1 Total Execution Time: 15.42 minutes\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nEXPERIMENT T2: INTER-AGENT CONSENSUS ANALYSIS\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSEED 1/3: 42\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Training Swarm-Mamba...\n    Epoch  1/5 | Loss: 0.8027 | Acc: 0.7239\n    Epoch  2/5 | Loss: 0.7280 | Acc: 0.7674\n    Epoch  3/5 | Loss: 0.6528 | Acc: 0.8188\n    Epoch  4/5 | Loss: 0.6109 | Acc: 0.8441\n    Epoch  5/5 | Loss: 0.5964 | Acc: 0.8449\n  Analyzing consensus patterns...\n\n  Seed 42 Results:\n    Overall Consensus Rate: 0.9845\n    Mean Inter-Agent Kappa: 0.8550\n    Consensus on Correct: 0.9891\n    Consensus on Incorrect: 0.9540\n    Agent Accuracies: ['0.8404', '0.8294', '0.8404', '0.8294']\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSEED 2/3: 123\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Training Swarm-Mamba...\n    Epoch  1/5 | Loss: 0.8467 | Acc: 0.7221\n    Epoch  2/5 | Loss: 0.6998 | Acc: 0.7927\n    Epoch  3/5 | Loss: 0.6626 | Acc: 0.8214\n    Epoch  4/5 | Loss: 0.6392 | Acc: 0.8197\n    Epoch  5/5 | Loss: 0.6280 | Acc: 0.8275\n  Analyzing consensus patterns...\n\n  Seed 123 Results:\n    Overall Consensus Rate: 0.9764\n    Mean Inter-Agent Kappa: 0.7236\n    Consensus on Correct: 0.9818\n    Consensus on Incorrect: 0.9423\n    Agent Accuracies: ['0.8239', '0.8294', '0.8183', '0.8202']\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSEED 3/3: 2024\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Training Swarm-Mamba...\n    Epoch  1/5 | Loss: 0.8381 | Acc: 0.7300\n    Epoch  2/5 | Loss: 0.7126 | Acc: 0.7892\n    Epoch  3/5 | Loss: 0.6495 | Acc: 0.8162\n    Epoch  4/5 | Loss: 0.5983 | Acc: 0.8415\n    Epoch  5/5 | Loss: 0.5794 | Acc: 0.8545\n  Analyzing consensus patterns...\n\n  Seed 2024 Results:\n    Overall Consensus Rate: 0.9845\n    Mean Inter-Agent Kappa: 0.8927\n    Consensus on Correct: 0.9875\n    Consensus on Incorrect: 0.9618\n    Agent Accuracies: ['0.8404', '0.8422', '0.8330', '0.8385']\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nT2 RESULTS SUMMARY\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nOverall Consensus Rate: 0.9818 ¬± 0.0038\nMean Inter-Agent Kappa: 0.8238 ¬± 0.0725\n\nConsensus on CORRECT predictions: 0.9861 ¬± 0.0031\nConsensus on INCORRECT predictions: 0.9527 ¬± 0.0080\n\nT2 Success Criterion (Consensus > 85% on correct): PASSED ‚úì\n\nConsensus Difference Test:\n  t-statistic: 5.5094\n  p-value: 0.0053\n  Significance: **\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nAGENT SPECIALIZATION ANALYSIS\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  Agent 1 Accuracy: 0.8349 ¬± 0.0078\n  Agent 2 Accuracy: 0.8336 ¬± 0.0061\n  Agent 3 Accuracy: 0.8306 ¬± 0.0092\n  Agent 4 Accuracy: 0.8294 ¬± 0.0075\n\n  Agent Diversity (œÉ of accuracies): 0.0022\n  Interpretation: Low diversity - agents similar\n\nT2 Total Execution Time: 3.99 minutes\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPART 1 EXECUTION COMPLETE\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nTotal Execution Time: 19.41 minutes (1164.9 seconds)\nPeak GPU Memory Used: 0.11 GB\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nEXPERIMENTS COMPLETED IN PART 1:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  ‚úì Parameter Verification (all models ~520K, within 1%)\n  ‚úì T1: Comparative Benchmarking (3 models √ó 3 seeds)\n  ‚úì T2: Inter-Agent Consensus Analysis (3 seeds)\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nPROCEED TO PART 2 FOR:\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  ‚Üí T3: Noise Robustness Testing\n  ‚Üí T4: Communication Ablation Study\n  ‚Üí T5: Inference Latency Benchmarking\n  ‚Üí Final Statistical Summary\n  ‚Üí Publication-Ready Tables\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSWARM-MAMBA: Q1 PUBLICATION CODE - PART 2 OF 2\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nOptimized for: Kaggle P100 GPU (16GB VRAM)\nContents:\n  - T3: Noise Robustness Testing\n  - T4: Communication Ablation Study\n  - T5: Inference Latency Benchmarking\n  - Final Statistical Summary\n  - Publication-Ready Tables\n  \nHardware Profile:\n  - GPU: NVIDIA P100 (16GB)\n  - Batch Size: 200 (optimized for memory)\n  - Epochs: 5 (balanced speed/quality)\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport gc\nimport warnings\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import (\n    roc_auc_score, f1_score, accuracy_score, \n    precision_score, recall_score, confusion_matrix,\n    cohen_kappa_score\n)\nfrom scipy import stats\n\nwarnings.filterwarnings('ignore')\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# IMPORT CONFIGURATION AND MODELS FROM PART 1\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@dataclass\nclass Config:\n    seeds: List[int] = field(default_factory=lambda: [42, 123, 2024])\n    data_root: str = \"/kaggle/input/breakhis-400x/BreaKHis 400X\"\n    img_size: int = 224\n    n_classes: int = 2\n    batch_size: int = 200\n    epochs: int = 5\n    lr: float = 3e-4\n    weight_decay: float = 0.01\n    d_model: int = 80\n    n_layers: int = 2\n    d_state: int = 16\n    n_agents: int = 4\n    n_comm_rounds: int = 2\n    patch_size: int = 16\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    num_workers: int = 2\n    pin_memory: bool = True\n\nCONFIG = Config()\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\ndef get_memory_stats():\n    if torch.cuda.is_available():\n        return {\n            'allocated': torch.cuda.memory_allocated() / 1024**3,\n            'reserved': torch.cuda.memory_reserved() / 1024**3,\n            'max': torch.cuda.max_memory_allocated() / 1024**3\n        }\n    return {'allocated': 0, 'reserved': 0, 'max': 0}\n\ndef clear_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(\"‚ïê\" * 70)\nprint(\"SWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS - PART 2\")\nprint(\"‚ïê\" * 70)\nprint(f\"Device: {CONFIG.device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(\"‚ïê\" * 70)\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# COPY ESSENTIAL CLASSES FROM PART 1\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass BreaKHisDataset(Dataset):\n    def __init__(self, root_dir: str, split: str = \"train\", transform=None):\n        self.root_dir = Path(root_dir)\n        self.split = split\n        self.transform = transform\n        self.samples = []\n        self.labels = []\n        self._load_data()\n        \n    def _load_data(self):\n        split_dir = self.root_dir / self.split\n        for class_idx, class_name in enumerate([\"benign\", \"malignant\"]):\n            class_dir = split_dir / class_name\n            if class_dir.exists():\n                for img_path in sorted(class_dir.glob(\"*.png\")):\n                    self.samples.append(str(img_path))\n                    self.labels.append(class_idx)\n        self.labels = np.array(self.labels)\n        \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img = Image.open(self.samples[idx]).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, self.labels[idx]\n\ndef get_transforms(is_train: bool = True):\n    if is_train:\n        return transforms.Compose([\n            transforms.Resize((CONFIG.img_size, CONFIG.img_size)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    return transforms.Compose([\n        transforms.Resize((CONFIG.img_size, CONFIG.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\nclass SelectiveSSM(nn.Module):\n    def __init__(self, d_model: int, d_state: int = 16, expand: int = 2):\n        super().__init__()\n        self.d_model = d_model\n        self.d_state = d_state\n        self.d_inner = d_model * expand\n        self.dt_rank = max(d_model // 16, 1)\n        \n        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, 4, padding=2, groups=self.d_inner)\n        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + d_state * 2, bias=False)\n        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n        \n        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n        self.A_log = nn.Parameter(torch.log(A))\n        self.D = nn.Parameter(torch.ones(self.d_inner))\n        \n        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n        self.norm = nn.LayerNorm(self.d_inner)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, L, _ = x.shape\n        xz = self.in_proj(x)\n        x_branch, z = xz.chunk(2, dim=-1)\n        x_branch = x_branch.transpose(1, 2)\n        x_branch = self.conv1d(x_branch)[:, :, :L]\n        x_branch = x_branch.transpose(1, 2)\n        x_branch = F.silu(x_branch)\n        x_dbl = self.x_proj(x_branch)\n        dt, B_param, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n        dt = F.softplus(self.dt_proj(dt))\n        A = -torch.exp(self.A_log)\n        y = self._scan_optimized(x_branch, dt, A, B_param, C)\n        y = self.norm(y)\n        y = y * F.silu(z)\n        return self.out_proj(y)\n    \n    def _scan_optimized(self, x, dt, A, B, C):\n        B_size, L, D = x.shape\n        chunk_size = 32\n        outputs = []\n        h = torch.zeros(B_size, D, self.d_state, device=x.device, dtype=x.dtype)\n        for start in range(0, L, chunk_size):\n            end = min(start + chunk_size, L)\n            chunk_out = []\n            for t in range(start, end):\n                dt_t = dt[:, t, :].unsqueeze(-1)\n                dA = torch.exp(A.view(1, 1, -1) * dt_t)\n                dB = dt_t * B[:, t, :].unsqueeze(1)\n                h = dA * h + dB * x[:, t, :].unsqueeze(-1)\n                y_t = (h * C[:, t, :].unsqueeze(1)).sum(-1) + self.D * x[:, t, :]\n                chunk_out.append(y_t)\n            outputs.extend(chunk_out)\n        return torch.stack(outputs, dim=1)\n\nclass MicroMamba(nn.Module):\n    def __init__(self, d_model: int, n_layers: int = 2, d_state: int = 16):\n        super().__init__()\n        self.d_model = d_model\n        self.blocks = nn.ModuleList([\n            nn.ModuleDict({\n                'norm': nn.LayerNorm(d_model),\n                'ssm': SelectiveSSM(d_model, d_state),\n                'drop': nn.Dropout(0.1)\n            }) for _ in range(n_layers)\n        ])\n        self.agent_embed = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n        \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B = x.shape[0]\n        x = torch.cat([self.agent_embed.expand(B, -1, -1), x], dim=1)\n        for block in self.blocks:\n            res = x\n            x = block['norm'](x)\n            x = block['ssm'](x)\n            x = block['drop'](x)\n            x = res + x\n        return x[:, 1:, :]\n\nclass MessagePassing(nn.Module):\n    def __init__(self, d_model: int, n_agents: int, n_heads: int = 4):\n        super().__init__()\n        self.n_agents = n_agents\n        self.d_model = d_model\n        self.d_compress = d_model // 2\n        self.compress = nn.Sequential(\n            nn.Linear(d_model, self.d_compress),\n            nn.GELU(),\n            nn.LayerNorm(self.d_compress)\n        )\n        self.decompress = nn.Linear(self.d_compress, d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=0.1, batch_first=True)\n        self.alpha = nn.Parameter(torch.tensor(0.3))\n        \n    def forward(self, states: List[torch.Tensor]) -> List[torch.Tensor]:\n        B = states[0].shape[0]\n        summaries = torch.stack([self.compress(s.mean(dim=1)) for s in states], dim=1)\n        decompressed = self.decompress(summaries)\n        attn_out, _ = self.attn(decompressed, decompressed, decompressed)\n        alpha = torch.sigmoid(self.alpha)\n        updated = []\n        for i, s in enumerate(states):\n            msg = attn_out[:, i:i+1, :].expand(-1, s.shape[1], -1)\n            updated.append(s + alpha * msg)\n        return updated\n\nclass Consensus(nn.Module):\n    def __init__(self, d_model: int, n_agents: int, n_classes: int):\n        super().__init__()\n        self.n_agents = n_agents\n        self.importance = nn.Parameter(torch.ones(n_agents) / n_agents)\n        self.agent_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.LayerNorm(d_model),\n                nn.Linear(d_model, d_model // 2),\n                nn.GELU(),\n                nn.Dropout(0.1),\n                nn.Linear(d_model // 2, n_classes)\n            ) for _ in range(n_agents)\n        ])\n        self.consensus_head = nn.Sequential(\n            nn.LayerNorm(d_model * n_agents),\n            nn.Linear(d_model * n_agents, d_model),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model, n_classes)\n        )\n        \n    def forward(self, states: List[torch.Tensor]) -> Tuple[torch.Tensor, List[torch.Tensor], Dict]:\n        pooled = [s.mean(dim=1) for s in states]\n        individual = [head(p) for head, p in zip(self.agent_heads, pooled)]\n        concat = torch.cat(pooled, dim=-1)\n        consensus_logits = self.consensus_head(concat)\n        \n        with torch.no_grad():\n            preds = torch.stack([torch.argmax(l, dim=-1) for l in individual], dim=1)\n            mode_pred = torch.mode(preds, dim=1).values\n            agreement = (preds == mode_pred.unsqueeze(1)).float().mean()\n            kappas = []\n            for i in range(len(individual)):\n                for j in range(i+1, len(individual)):\n                    p_i = torch.argmax(individual[i], dim=-1).cpu().numpy()\n                    p_j = torch.argmax(individual[j], dim=-1).cpu().numpy()\n                    if len(np.unique(p_i)) > 1 and len(np.unique(p_j)) > 1:\n                        try:\n                            kappas.append(cohen_kappa_score(p_i, p_j))\n                        except:\n                            pass\n        \n        metrics = {\n            'consensus_rate': agreement.item(),\n            'mean_kappa': np.mean(kappas) if kappas else 1.0\n        }\n        return consensus_logits, individual, metrics\n\nclass SwarmMamba(nn.Module):\n    def __init__(self, img_size: int = 224, patch_size: int = 16, in_chans: int = 3,\n                 n_agents: int = 4, d_model: int = 80, n_layers: int = 2,\n                 n_classes: int = 2, n_comm_rounds: int = 2, d_state: int = 16,\n                 use_communication: bool = True):\n        super().__init__()\n        self.n_agents = n_agents\n        self.n_comm_rounds = n_comm_rounds\n        self.use_communication = use_communication\n        self.n_patches = (img_size // patch_size) ** 2\n        self.patch_embed = nn.Conv2d(in_chans, d_model, patch_size, patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, d_model) * 0.02)\n        self.agents = nn.ModuleList([\n            MicroMamba(d_model, n_layers, d_state) for _ in range(n_agents)\n        ])\n        self.message_passing = MessagePassing(d_model, n_agents)\n        self.consensus = Consensus(d_model, n_agents, n_classes)\n        \n    def partition(self, x: torch.Tensor) -> List[torch.Tensor]:\n        B, N, D = x.shape\n        per_agent = N // self.n_agents\n        return [x[:, i*per_agent:(i+1)*per_agent if i < self.n_agents-1 else N, :].clone() \n                for i in range(self.n_agents)]\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor], Dict]:\n        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n        x = x + self.pos_embed\n        partitions = self.partition(x)\n        states = [agent(part) for agent, part in zip(self.agents, partitions)]\n        for _ in range(self.n_comm_rounds):\n            if self.use_communication:\n                states = self.message_passing(states)\n            states = [agent(s) for agent, s in zip(self.agents, states)]\n        return self.consensus(states)\n\nclass VisionMamba(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, d_model=82, \n                 n_layers=9, n_classes=2, d_state=16):\n        super().__init__()\n        self.n_patches = (img_size // patch_size) ** 2\n        self.patch_embed = nn.Conv2d(in_chans, d_model, patch_size, patch_size)\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches, d_model) * 0.02)\n        self.blocks = nn.ModuleList([\n            nn.ModuleDict({\n                'norm': nn.LayerNorm(d_model),\n                'ssm': SelectiveSSM(d_model, d_state)\n            }) for _ in range(n_layers)\n        ])\n        self.head = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(d_model // 2, n_classes)\n        )\n        \n    def forward(self, x):\n        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n        x = x + self.pos_embed\n        for block in self.blocks:\n            x = x + block['ssm'](block['norm'](x))\n        return self.head(x.mean(dim=1))\n\nclass Timer:\n    def __init__(self):\n        self.times = {}\n        \n    def start(self, name):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.times[name] = {'start': time.perf_counter()}\n        \n    def stop(self, name):\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        self.times[name]['end'] = time.perf_counter()\n        self.times[name]['duration'] = self.times[name]['end'] - self.times[name]['start']\n        return self.times[name]['duration']\n\ndef train_epoch(model, loader, optimizer, criterion, device, is_swarm=False):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for batch_x, batch_y in loader:\n        batch_x, batch_y = batch_x.to(device, non_blocking=True), batch_y.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        if is_swarm:\n            logits, individual, _ = model(batch_x)\n            loss = criterion(logits, batch_y)\n            for ind in individual:\n                loss += 0.1 * criterion(ind, batch_y)\n        else:\n            logits = model(batch_x)\n            loss = criterion(logits, batch_y)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        total_loss += loss.item() * batch_x.size(0)\n        preds = torch.argmax(logits, dim=-1)\n        correct += (preds == batch_y).sum().item()\n        total += batch_y.size(0)\n    return total_loss / total, correct / total\n\n@torch.no_grad()\ndef evaluate(model, loader, device, is_swarm=False):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    for batch_x, batch_y in loader:\n        batch_x, batch_y = batch_x.to(device, non_blocking=True), batch_y.to(device, non_blocking=True)\n        if is_swarm:\n            logits, _, _ = model(batch_x)\n        else:\n            logits = model(batch_x)\n        probs = F.softmax(logits, dim=-1)\n        preds = torch.argmax(logits, dim=-1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(batch_y.cpu().numpy())\n        all_probs.extend(probs[:, 1].cpu().numpy())\n    \n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n    \n    return {\n        'accuracy': accuracy_score(all_labels, all_preds),\n        'auc': roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.5,\n        'f1': f1_score(all_labels, all_preds, average='weighted')\n    }\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION T3: NOISE ROBUSTNESS TESTING\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nclass NoisyTransform:\n    \"\"\"Apply Gaussian noise to images\"\"\"\n    def __init__(self, noise_std: float = 0.1):\n        self.noise_std = noise_std\n        \n    def __call__(self, tensor):\n        noise = torch.randn_like(tensor) * self.noise_std\n        return torch.clamp(tensor + noise, 0, 1)\n\ndef run_T3_noise_robustness():\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"EXPERIMENT T3: NOISE ROBUSTNESS TESTING\")\n    print(\"‚ïê\" * 70)\n    \n    timer = Timer()\n    timer.start('T3_total')\n    \n    noise_levels = [0.0, 0.05, 0.10, 0.15, 0.20]\n    \n    results = {\n        'Swarm-Mamba': {level: [] for level in noise_levels},\n        'Vision-Mamba': {level: [] for level in noise_levels}\n    }\n    \n    test_ds = BreaKHisDataset(CONFIG.data_root, \"test\", get_transforms(False))\n    \n    for seed in CONFIG.seeds:\n        print(f\"\\n{'‚îÄ' * 50}\")\n        print(f\"Seed: {seed}\")\n        print('‚îÄ' * 50)\n        set_seed(seed)\n        clear_memory()\n        \n        train_ds = BreaKHisDataset(CONFIG.data_root, \"train\", get_transforms(True))\n        train_loader = DataLoader(train_ds, batch_size=CONFIG.batch_size, shuffle=True,\n                                 num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n        \n        for model_name in ['Swarm-Mamba', 'Vision-Mamba']:\n            print(f\"\\n  Training {model_name}...\")\n            \n            if model_name == 'Swarm-Mamba':\n                model = SwarmMamba(\n                    img_size=CONFIG.img_size, patch_size=CONFIG.patch_size,\n                    n_agents=CONFIG.n_agents, d_model=CONFIG.d_model,\n                    n_layers=CONFIG.n_layers, n_classes=CONFIG.n_classes,\n                    n_comm_rounds=CONFIG.n_comm_rounds, d_state=CONFIG.d_state\n                ).to(CONFIG.device)\n                is_swarm = True\n            else:\n                model = VisionMamba(\n                    img_size=CONFIG.img_size, patch_size=CONFIG.patch_size,\n                    d_model=82, n_layers=9, n_classes=CONFIG.n_classes, d_state=CONFIG.d_state\n                ).to(CONFIG.device)\n                is_swarm = False\n            \n            optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.lr, weight_decay=CONFIG.weight_decay)\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, CONFIG.epochs)\n            criterion = nn.CrossEntropyLoss()\n            \n            for epoch in range(CONFIG.epochs):\n                train_epoch(model, train_loader, optimizer, criterion, CONFIG.device, is_swarm)\n                scheduler.step()\n            \n            for noise_level in noise_levels:\n                if noise_level == 0:\n                    test_transform = get_transforms(False)\n                else:\n                    test_transform = transforms.Compose([\n                        transforms.Resize((CONFIG.img_size, CONFIG.img_size)),\n                        transforms.ToTensor(),\n                        NoisyTransform(noise_level),\n                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                    ])\n                \n                noisy_test_ds = BreaKHisDataset(CONFIG.data_root, \"test\", test_transform)\n                test_loader = DataLoader(noisy_test_ds, batch_size=CONFIG.batch_size, shuffle=False,\n                                         num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n                \n                result = evaluate(model, test_loader, CONFIG.device, is_swarm)\n                results[model_name][noise_level].append(result['accuracy'])\n                \n                print(f\"    Noise œÉ={noise_level:.2f}: Acc={result['accuracy']:.4f}\")\n            \n            del model\n            clear_memory()\n    \n    t3_time = timer.stop('T3_total')\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"T3 RESULTS SUMMARY\")\n    print(\"‚ïê\" * 70)\n    \n    print(f\"\\n{'Model':<15} {'Clean':>12} {'œÉ=0.05':>12} {'œÉ=0.10':>12} {'œÉ=0.15':>12} {'œÉ=0.20':>12} {'Degradation':>12}\")\n    print(\"‚îÄ\" * 90)\n    \n    for model_name in ['Swarm-Mamba', 'Vision-Mamba']:\n        clean_acc = np.mean(results[model_name][0.0])\n        worst_acc = np.mean(results[model_name][0.20])\n        degradation = (clean_acc - worst_acc) / clean_acc * 100\n        \n        row = f\"{model_name:<15}\"\n        for noise in noise_levels:\n            acc_mean = np.mean(results[model_name][noise])\n            acc_std = np.std(results[model_name][noise])\n            row += f\" {acc_mean:.3f}¬±{acc_std:.3f}\"\n        row += f\"  {degradation:>10.1f}%\"\n        print(row)\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"ROBUSTNESS COMPARISON\")\n    print(\"‚îÄ\" * 70)\n    \n    swarm_deg = (np.mean(results['Swarm-Mamba'][0.0]) - np.mean(results['Swarm-Mamba'][0.20])) / np.mean(results['Swarm-Mamba'][0.0]) * 100\n    vim_deg = (np.mean(results['Vision-Mamba'][0.0]) - np.mean(results['Vision-Mamba'][0.20])) / np.mean(results['Vision-Mamba'][0.0]) * 100\n    \n    print(f\"Swarm-Mamba Degradation: {swarm_deg:.2f}%\")\n    print(f\"Vision-Mamba Degradation: {vim_deg:.2f}%\")\n    \n    if swarm_deg < vim_deg:\n        improvement = vim_deg - swarm_deg\n        print(f\"‚úì Swarm-Mamba is {improvement:.2f}% more robust\")\n    else:\n        print(f\"‚úó Vision-Mamba is more robust\")\n    \n    print(f\"\\nT3 Total Execution Time: {t3_time/60:.2f} minutes\")\n    \n    return results\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION T4: COMMUNICATION ABLATION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef run_T4_communication_ablation():\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"EXPERIMENT T4: COMMUNICATION ABLATION STUDY\")\n    print(\"‚ïê\" * 70)\n    \n    timer = Timer()\n    timer.start('T4_total')\n    \n    results = {\n        'With Communication': [],\n        'Without Communication': []\n    }\n    \n    for seed in CONFIG.seeds:\n        print(f\"\\n{'‚îÄ' * 50}\")\n        print(f\"Seed: {seed}\")\n        print('‚îÄ' * 50)\n        set_seed(seed)\n        clear_memory()\n        \n        train_ds = BreaKHisDataset(CONFIG.data_root, \"train\", get_transforms(True))\n        test_ds = BreaKHisDataset(CONFIG.data_root, \"test\", get_transforms(False))\n        \n        train_loader = DataLoader(train_ds, batch_size=CONFIG.batch_size, shuffle=True,\n                                 num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n        test_loader = DataLoader(test_ds, batch_size=CONFIG.batch_size, shuffle=False,\n                                num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n        \n        for use_comm, name in [(True, 'With Communication'), (False, 'Without Communication')]:\n            print(f\"\\n  Training Swarm-Mamba {name}...\")\n            \n            model = SwarmMamba(\n                img_size=CONFIG.img_size, patch_size=CONFIG.patch_size,\n                n_agents=CONFIG.n_agents, d_model=CONFIG.d_model,\n                n_layers=CONFIG.n_layers, n_classes=CONFIG.n_classes,\n                n_comm_rounds=CONFIG.n_comm_rounds, d_state=CONFIG.d_state,\n                use_communication=use_comm\n            ).to(CONFIG.device)\n            \n            optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG.lr, weight_decay=CONFIG.weight_decay)\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, CONFIG.epochs)\n            criterion = nn.CrossEntropyLoss()\n            \n            for epoch in range(CONFIG.epochs):\n                train_epoch(model, train_loader, optimizer, criterion, CONFIG.device, is_swarm=True)\n                scheduler.step()\n            \n            result = evaluate(model, test_loader, CONFIG.device, is_swarm=True)\n            results[name].append(result['accuracy'])\n            \n            print(f\"    Accuracy: {result['accuracy']:.4f}\")\n            \n            del model\n            clear_memory()\n    \n    t4_time = timer.stop('T4_total')\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"T4 RESULTS SUMMARY\")\n    print(\"‚ïê\" * 70)\n    \n    with_comm = np.array(results['With Communication'])\n    without_comm = np.array(results['Without Communication'])\n    \n    print(f\"\\nWith Communication:    {np.mean(with_comm):.4f} ¬± {np.std(with_comm):.4f}\")\n    print(f\"Without Communication: {np.mean(without_comm):.4f} ¬± {np.std(without_comm):.4f}\")\n    \n    t_stat, p_val = stats.ttest_rel(with_comm, without_comm)\n    improvement = (np.mean(with_comm) - np.mean(without_comm)) / np.mean(without_comm) * 100\n    \n    print(f\"\\nPaired t-test:\")\n    print(f\"  t-statistic: {t_stat:.4f}\")\n    print(f\"  p-value: {p_val:.4f}\")\n    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"\"\n    print(f\"  Significance: {sig if sig else 'Not significant'}\")\n    print(f\"  Improvement: {improvement:.2f}%\")\n    \n    if p_val < 0.05:\n        print(f\"\\n‚úì Communication provides statistically significant improvement\")\n    else:\n        print(f\"\\n‚úó No significant difference found\")\n    \n    print(f\"\\nT4 Total Execution Time: {t4_time/60:.2f} minutes\")\n    \n    return results\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# SECTION T5: DETAILED INFERENCE LATENCY\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n@torch.no_grad()\ndef detailed_inference_benchmark(model, loader, device, is_swarm=False):\n    model.eval()\n    batch_x, _ = next(iter(loader))\n    batch_x = batch_x.to(device)\n    \n    # Warmup\n    for _ in range(10):\n        if is_swarm:\n            _ = model(batch_x)\n        else:\n            _ = model(batch_x)\n    \n    if torch.cuda.is_available():\n        torch.cuda.synchronize()\n    \n    # Measure\n    times = []\n    for _ in range(100):\n        start = time.perf_counter()\n        if is_swarm:\n            _ = model(batch_x)\n        else:\n            _ = model(batch_x)\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        times.append(time.perf_counter() - start)\n    \n    times = np.array(times) * 1000  # Convert to ms\n    \n    return {\n        'mean': np.mean(times),\n        'std': np.std(times),\n        'min': np.min(times),\n        'max': np.max(times),\n        'median': np.median(times),\n        'p95': np.percentile(times, 95),\n        'p99': np.percentile(times, 99),\n        'per_sample': np.mean(times) / batch_x.size(0),\n        'throughput': batch_x.size(0) / (np.mean(times) / 1000)\n    }\n\ndef run_T5_latency_benchmarking():\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"EXPERIMENT T5: DETAILED INFERENCE LATENCY BENCHMARKING\")\n    print(\"‚ïê\" * 70)\n    \n    timer = Timer()\n    timer.start('T5_total')\n    \n    set_seed(42)\n    \n    test_ds = BreaKHisDataset(CONFIG.data_root, \"test\", get_transforms(False))\n    test_loader = DataLoader(test_ds, batch_size=CONFIG.batch_size, shuffle=False,\n                            num_workers=CONFIG.num_workers, pin_memory=CONFIG.pin_memory)\n    \n    models = {\n        'Swarm-Mamba': SwarmMamba(\n            img_size=CONFIG.img_size, patch_size=CONFIG.patch_size,\n            n_agents=CONFIG.n_agents, d_model=CONFIG.d_model,\n            n_layers=CONFIG.n_layers, n_classes=CONFIG.n_classes,\n            n_comm_rounds=CONFIG.n_comm_rounds, d_state=CONFIG.d_state\n        ),\n        'Vision-Mamba': VisionMamba(\n            img_size=CONFIG.img_size, patch_size=CONFIG.patch_size,\n            d_model=82, n_layers=9, n_classes=CONFIG.n_classes, d_state=CONFIG.d_state\n        )\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        print(f\"\\nBenchmarking {name}...\")\n        model = model.to(CONFIG.device)\n        is_swarm = 'Swarm' in name\n        \n        bench = detailed_inference_benchmark(model, test_loader, CONFIG.device, is_swarm)\n        results[name] = bench\n        \n        print(f\"  Batch Time (ms):  {bench['mean']:.2f} ¬± {bench['std']:.2f}\")\n        print(f\"  Per-Sample (ms):  {bench['per_sample']:.3f}\")\n        print(f\"  Throughput:       {bench['throughput']:.1f} samples/s\")\n        print(f\"  Median:           {bench['median']:.2f} ms\")\n        print(f\"  95th percentile:  {bench['p95']:.2f} ms\")\n        print(f\"  99th percentile:  {bench['p99']:.2f} ms\")\n        \n        del model\n        clear_memory()\n    \n    t5_time = timer.stop('T5_total')\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"T5 SUMMARY TABLE\")\n    print(\"‚ïê\" * 70)\n    \n    print(f\"\\n{'Metric':<20} {'Swarm-Mamba':>15} {'Vision-Mamba':>15} {'Speedup':>10}\")\n    print(\"‚îÄ\" * 65)\n    \n    metrics = [\n        ('Batch Time (ms)', 'mean'),\n        ('Per-Sample (ms)', 'per_sample'),\n        ('Throughput (samp/s)', 'throughput'),\n        ('P95 Latency (ms)', 'p95'),\n        ('P99 Latency (ms)', 'p99')\n    ]\n    \n    for label, key in metrics:\n        swarm_val = results['Swarm-Mamba'][key]\n        vim_val = results['Vision-Mamba'][key]\n        if 'throughput' in key.lower():\n            speedup = swarm_val / vim_val\n        else:\n            speedup = vim_val / swarm_val\n        print(f\"{label:<20} {swarm_val:>15.2f} {vim_val:>15.2f} {speedup:>9.2f}x\")\n    \n    print(f\"\\nT5 Total Execution Time: {t5_time/60:.2f} minutes\")\n    \n    return results\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# FINAL PUBLICATION TABLES\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\ndef generate_publication_tables(t3_results, t4_results, t5_results):\n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"PUBLICATION-READY SUMMARY TABLES\")\n    print(\"‚ïê\" * 70)\n    \n    # TABLE 1: Main Comparison (from Part 1 results - hardcoded from output)\n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"TABLE 1: Comparative Performance (Parameter-Matched ~520K)\")\n    print(\"‚îÄ\" * 70)\n    \n    print(f\"\\n{'Model':<15} {'Params':>10} {'AUC':>15} {'Accuracy':>15} {'F1':>15}\")\n    print(\"‚îÄ\" * 70)\n    print(f\"{'Swarm-Mamba':<15} {'524,343':>10} {'0.8128¬±0.0061':>15} {'0.8391¬±0.0046':>15} {'0.8300¬±0.0063':>15}\")\n    print(f\"{'Vision-Mamba':<15} {'522,773':>10} {'0.8030¬±0.0018':>15} {'0.8483¬±0.0048':>15} {'0.8419¬±0.0054':>15}\")\n    print(f\"{'ViT':<15} {'520,814':>10} {'0.7832¬±0.0009':>15} {'0.8251¬±0.0090':>15} {'0.8193¬±0.0085':>15}\")\n    \n    # TABLE 2: Consensus Analysis\n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"TABLE 2: Agent Consensus Analysis\")\n    print(\"‚îÄ\" * 70)\n    print(\"\\nMetric                          Value\")\n    print(\"‚îÄ\" * 45)\n    print(f\"Overall Consensus Rate          98.18% ¬± 0.38%\")\n    print(f\"Inter-Agent Kappa               0.8238 ¬± 0.0725\")\n    print(f\"Consensus on Correct            98.61% ¬± 0.31%\")\n    print(f\"Consensus on Incorrect          95.27% ¬± 0.80%\")\n    print(f\"Agent Diversity (œÉ)             0.0022\")\n    \n    # TABLE 3: Noise Robustness\n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"TABLE 3: Noise Robustness Comparison\")\n    print(\"‚îÄ\" * 70)\n    \n    print(f\"\\n{'Model':<15} {'Clean':>10} {'œÉ=0.05':>10} {'œÉ=0.10':>10} {'œÉ=0.15':>10} {'œÉ=0.20':>10} {'Deg.':>8}\")\n    print(\"‚îÄ\" * 75)\n    \n    for model_name in ['Swarm-Mamba', 'Vision-Mamba']:\n        clean = np.mean(t3_results[model_name][0.0])\n        worst = np.mean(t3_results[model_name][0.20])\n        deg = (clean - worst) / clean * 100\n        \n        row = f\"{model_name:<15}\"\n        for noise in [0.0, 0.05, 0.10, 0.15, 0.20]:\n            acc = np.mean(t3_results[model_name][noise])\n            row += f\" {acc:>9.3f}\"\n        row += f\" {deg:>7.1f}%\"\n        print(row)\n    \n    # TABLE 4: Communication Ablation\n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"TABLE 4: Communication Ablation\")\n    print(\"‚îÄ\" * 70)\n    \n    with_comm = np.array(t4_results['With Communication'])\n    without_comm = np.array(t4_results['Without Communication'])\n    improvement = (np.mean(with_comm) - np.mean(without_comm)) / np.mean(without_comm) * 100\n    t_stat, p_val = stats.ttest_rel(with_comm, without_comm)\n    \n    print(f\"\\n{'Configuration':<25} {'Accuracy':>15} {'Œî vs No-Comm':>15} {'p-value':>10}\")\n    print(\"‚îÄ\" * 70)\n    print(f\"{'With Communication':<25} {np.mean(with_comm):.4f}¬±{np.std(with_comm):.4f}  {'+'+str(round(improvement, 2))+'%':>15} {p_val:>10.4f}\")\n    print(f\"{'Without Communication':<25} {np.mean(without_comm):.4f}¬±{np.std(without_comm):.4f}  {'baseline':>15} {'-':>10}\")\n    \n    # TABLE 5: Latency Comparison\n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"TABLE 5: Inference Latency Analysis\")\n    print(\"‚îÄ\" * 70)\n    \n    print(f\"\\n{'Metric':<25} {'Swarm-Mamba':>15} {'Vision-Mamba':>15} {'Relative':>10}\")\n    print(\"‚îÄ\" * 70)\n    \n    swarm_time = t5_results['Swarm-Mamba']['per_sample']\n    vim_time = t5_results['Vision-Mamba']['per_sample']\n    swarm_tp = t5_results['Swarm-Mamba']['throughput']\n    vim_tp = t5_results['Vision-Mamba']['throughput']\n    \n    print(f\"{'Per-Sample Latency (ms)':<25} {swarm_time:>15.3f} {vim_time:>15.3f} {vim_time/swarm_time:>9.2f}x\")\n    print(f\"{'Throughput (samples/s)':<25} {swarm_tp:>15.1f} {vim_tp:>15.1f} {swarm_tp/vim_tp:>9.2f}x\")\n    print(f\"{'P95 Latency (ms)':<25} {t5_results['Swarm-Mamba']['p95']:>15.2f} {t5_results['Vision-Mamba']['p95']:>15.2f} {t5_results['Vision-Mamba']['p95']/t5_results['Swarm-Mamba']['p95']:>9.2f}x\")\n\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n# MAIN EXECUTION\n# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nif __name__ == \"__main__\":\n    print(\"\\n\" + \"‚ñà\" * 70)\n    print(\"‚ñà\" + \" \" * 68 + \"‚ñà\")\n    print(\"‚ñà\" + \"   SWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS\".center(68) + \"‚ñà\")\n    print(\"‚ñà\" + \"   PART 2: Advanced Experiments & Analysis\".center(68) + \"‚ñà\")\n    print(\"‚ñà\" + \" \" * 68 + \"‚ñà\")\n    print(\"‚ñà\" * 70)\n    \n    total_timer = Timer()\n    total_timer.start('total')\n    \n    # Check data\n    data_path = Path(CONFIG.data_root)\n    if not data_path.exists():\n        print(f\"\\nERROR: Data not found at {CONFIG.data_root}\")\n        sys.exit(1)\n    \n    # Run experiments\n    print(\"\\n\" + \"‚ñà\" * 70)\n    t3_results = run_T3_noise_robustness()\n    \n    print(\"\\n\" + \"‚ñà\" * 70)\n    t4_results = run_T4_communication_ablation()\n    \n    print(\"\\n\" + \"‚ñà\" * 70)\n    t5_results = run_T5_latency_benchmarking()\n    \n    # Generate publication tables\n    print(\"\\n\" + \"‚ñà\" * 70)\n    generate_publication_tables(t3_results, t4_results, t5_results)\n    \n    # Final summary\n    total_time = total_timer.stop('total')\n    final_mem = get_memory_stats()\n    \n    print(\"\\n\" + \"‚ïê\" * 70)\n    print(\"PART 2 EXECUTION COMPLETE\")\n    print(\"‚ïê\" * 70)\n    print(f\"\\nTotal Execution Time: {total_time/60:.2f} minutes ({total_time:.1f} seconds)\")\n    print(f\"Peak GPU Memory Used: {final_mem['max']:.2f} GB\")\n    \n    print(\"\\n\" + \"‚îÄ\" * 70)\n    print(\"ALL EXPERIMENTS COMPLETED\")\n    print(\"‚îÄ\" * 70)\n    print(\"  ‚úì T1: Comparative Benchmarking (Part 1)\")\n    print(\"  ‚úì T2: Consensus Analysis (Part 1)\")\n    print(\"  ‚úì T3: Noise Robustness Testing\")\n    print(\"  ‚úì T4: Communication Ablation Study\")\n    print(\"  ‚úì T5: Inference Latency Benchmarking\")\n    print(\"  ‚úì Publication-Ready Tables Generated\")\n    print(\"‚ïê\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T00:35:17.796062Z","iopub.execute_input":"2026-02-07T00:35:17.796685Z","iopub.status.idle":"2026-02-07T00:56:26.520788Z","shell.execute_reply.started":"2026-02-07T00:35:17.796650Z","shell.execute_reply":"2026-02-07T00:56:26.520018Z"}},"outputs":[{"name":"stdout","text":"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nSWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS - PART 2\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nDevice: cuda\nGPU: Tesla P100-PCIE-16GB\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n‚ñà                                                                    ‚ñà\n‚ñà                SWARM-MAMBA: Q1 PUBLICATION EXPERIMENTS             ‚ñà\n‚ñà                PART 2: Advanced Experiments & Analysis             ‚ñà\n‚ñà                                                                    ‚ñà\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nEXPERIMENT T3: NOISE ROBUSTNESS TESTING\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSeed: 42\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Training Swarm-Mamba...\n    Noise œÉ=0.00: Acc=0.8404\n    Noise œÉ=0.05: Acc=0.8422\n    Noise œÉ=0.10: Acc=0.8404\n    Noise œÉ=0.15: Acc=0.8404\n    Noise œÉ=0.20: Acc=0.8349\n\n  Training Vision-Mamba...\n    Noise œÉ=0.00: Acc=0.8385\n    Noise œÉ=0.05: Acc=0.8385\n    Noise œÉ=0.10: Acc=0.8385\n    Noise œÉ=0.15: Acc=0.8385\n    Noise œÉ=0.20: Acc=0.8294\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSeed: 123\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Training Swarm-Mamba...\n    Noise œÉ=0.00: Acc=0.8330\n    Noise œÉ=0.05: Acc=0.8349\n    Noise œÉ=0.10: Acc=0.8349\n    Noise œÉ=0.15: Acc=0.8330\n    Noise œÉ=0.20: Acc=0.8367\n\n  Training Vision-Mamba...\n    Noise œÉ=0.00: Acc=0.8495\n    Noise œÉ=0.05: Acc=0.8514\n    Noise œÉ=0.10: Acc=0.8459\n    Noise œÉ=0.15: Acc=0.8440\n    Noise œÉ=0.20: Acc=0.8294\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSeed: 2024\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Training Swarm-Mamba...\n    Noise œÉ=0.00: Acc=0.8440\n    Noise œÉ=0.05: Acc=0.8440\n    Noise œÉ=0.10: Acc=0.8440\n    Noise œÉ=0.15: Acc=0.8404\n    Noise œÉ=0.20: Acc=0.8367\n\n  Training Vision-Mamba...\n    Noise œÉ=0.00: Acc=0.8385\n    Noise œÉ=0.05: Acc=0.8257\n    Noise œÉ=0.10: Acc=0.8202\n    Noise œÉ=0.15: Acc=0.8183\n    Noise œÉ=0.20: Acc=0.8110\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nT3 RESULTS SUMMARY\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nModel                  Clean       œÉ=0.05       œÉ=0.10       œÉ=0.15       œÉ=0.20  Degradation\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba     0.839¬±0.005 0.840¬±0.004 0.840¬±0.004 0.838¬±0.003 0.836¬±0.001         0.4%\nVision-Mamba    0.842¬±0.005 0.839¬±0.010 0.835¬±0.011 0.834¬±0.011 0.823¬±0.009         2.3%\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nROBUSTNESS COMPARISON\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba Degradation: 0.36%\nVision-Mamba Degradation: 2.25%\n‚úì Swarm-Mamba is 1.89% more robust\n\nT3 Total Execution Time: 12.15 minutes\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nEXPERIMENT T4: COMMUNICATION ABLATION STUDY\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSeed: 42\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Training Swarm-Mamba With Communication...\n    Accuracy: 0.8404\n\n  Training Swarm-Mamba Without Communication...\n    Accuracy: 0.8495\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSeed: 123\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Training Swarm-Mamba With Communication...\n    Accuracy: 0.8330\n\n  Training Swarm-Mamba Without Communication...\n    Accuracy: 0.8239\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSeed: 2024\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  Training Swarm-Mamba With Communication...\n    Accuracy: 0.8440\n\n  Training Swarm-Mamba Without Communication...\n    Accuracy: 0.8422\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nT4 RESULTS SUMMARY\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nWith Communication:    0.8391 ¬± 0.0046\nWithout Communication: 0.8385 ¬± 0.0108\n\nPaired t-test:\n  t-statistic: 0.1147\n  p-value: 0.9192\n  Significance: Not significant\n  Improvement: 0.07%\n\n‚úó No significant difference found\n\nT4 Total Execution Time: 7.88 minutes\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nEXPERIMENT T5: DETAILED INFERENCE LATENCY BENCHMARKING\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nBenchmarking Swarm-Mamba...\n  Batch Time (ms):  217.80 ¬± 4.67\n  Per-Sample (ms):  1.089\n  Throughput:       918.3 samples/s\n  Median:           217.07 ms\n  95th percentile:  227.22 ms\n  99th percentile:  234.21 ms\n\nBenchmarking Vision-Mamba...\n  Batch Time (ms):  291.04 ¬± 5.89\n  Per-Sample (ms):  1.455\n  Throughput:       687.2 samples/s\n  Median:           289.27 ms\n  95th percentile:  303.45 ms\n  99th percentile:  317.37 ms\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nT5 SUMMARY TABLE\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nMetric                   Swarm-Mamba    Vision-Mamba    Speedup\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nBatch Time (ms)               217.80          291.04      1.34x\nPer-Sample (ms)                 1.09            1.46      1.34x\nThroughput (samp/s)           918.26          687.19      1.34x\nP95 Latency (ms)              227.22          303.45      1.34x\nP99 Latency (ms)              234.21          317.37      1.36x\n\nT5 Total Execution Time: 1.11 minutes\n\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPUBLICATION-READY SUMMARY TABLES\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTABLE 1: Comparative Performance (Parameter-Matched ~520K)\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nModel               Params             AUC        Accuracy              F1\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba        524,343   0.8128¬±0.0061   0.8391¬±0.0046   0.8300¬±0.0063\nVision-Mamba       522,773   0.8030¬±0.0018   0.8483¬±0.0048   0.8419¬±0.0054\nViT                520,814   0.7832¬±0.0009   0.8251¬±0.0090   0.8193¬±0.0085\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTABLE 2: Agent Consensus Analysis\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nMetric                          Value\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nOverall Consensus Rate          98.18% ¬± 0.38%\nInter-Agent Kappa               0.8238 ¬± 0.0725\nConsensus on Correct            98.61% ¬± 0.31%\nConsensus on Incorrect          95.27% ¬± 0.80%\nAgent Diversity (œÉ)             0.0022\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTABLE 3: Noise Robustness Comparison\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nModel                Clean     œÉ=0.05     œÉ=0.10     œÉ=0.15     œÉ=0.20     Deg.\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nSwarm-Mamba         0.839     0.840     0.840     0.838     0.836     0.4%\nVision-Mamba        0.842     0.839     0.835     0.834     0.823     2.3%\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTABLE 4: Communication Ablation\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nConfiguration                    Accuracy    Œî vs No-Comm    p-value\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nWith Communication        0.8391¬±0.0046           +0.07%     0.9192\nWithout Communication     0.8385¬±0.0108         baseline          -\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nTABLE 5: Inference Latency Analysis\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nMetric                        Swarm-Mamba    Vision-Mamba   Relative\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nPer-Sample Latency (ms)             1.089           1.455      1.34x\nThroughput (samples/s)              918.3           687.2      1.34x\nP95 Latency (ms)                   227.22          303.45      1.34x\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPART 2 EXECUTION COMPLETE\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nTotal Execution Time: 21.14 minutes (1268.6 seconds)\nPeak GPU Memory Used: 0.02 GB\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nALL EXPERIMENTS COMPLETED\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  ‚úì T1: Comparative Benchmarking (Part 1)\n  ‚úì T2: Consensus Analysis (Part 1)\n  ‚úì T3: Noise Robustness Testing\n  ‚úì T4: Communication Ablation Study\n  ‚úì T5: Inference Latency Benchmarking\n  ‚úì Publication-Ready Tables Generated\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n","output_type":"stream"}],"execution_count":2}]}